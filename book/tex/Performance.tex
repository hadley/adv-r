\hypertarget{performance}{%
\chapter{Performance}\label{performance}}

R is not a fast language. This is not an accident. R was purposely
designed to make data analysis and statistics easier for you to do. It
was not designed to make life easier for your computer. While R is slow
compared to other programming languages, for most purposes, it's fast
enough. \index{performance}

The goal of this part of the book is to give you a deeper understanding
of R's performance characteristics. In this chapter, you'll learn about
some of the trade-offs that R has made, valuing flexibility over
performance. The following four chapters will give you the skills to
improve the speed of your code when you need to:

\begin{itemize}
\item
  In \protect\hyperlink{profiling}{Profiling}, you'll learn how to
  systematically make your code faster. First you figure what's slow,
  and then you apply some general techniques to make the slow parts
  faster.
\item
  In \protect\hyperlink{memory}{Memory}, you'll learn about how R uses
  memory, and how garbage collection and copy-on-modify affect
  performance and memory usage.
\item
  For really high-performance code, you can move outside of R and use
  another programming language. \protect\hyperlink{rcpp}{Rcpp} will
  teach you the absolute minimum you need to know about C++ so you can
  write fast code using the Rcpp package.
\item
  To really understand the performance of built-in base functions,
  you'll need to learn a little bit about R's C API. In
  \protect\hyperlink{c-api}{R's C interface}, you'll learn a little
  about R's C internals.
\end{itemize}

Let's get started by learning more about why R is slow.

\hypertarget{why-is-r-slow}{%
\section{Why is R slow?}\label{why-is-r-slow}}

To understand R's performance, it helps to think about R as both a
language and as an implementation of that language. The R-language is
abstract: it defines what R code means and how it should work. The
implementation is concrete: it reads R code and computes a result. The
most popular implementation is the one from
\href{http://r-project.org}{r-project.org}. I'll call that
implementation GNU-R to distinguish it from R-language, and from the
other implementations I'll discuss later in the chapter.
\index{language definition}

The distinction between R-language and GNU-R is a bit murky because the
R-language is not formally defined. While there is the
\href{http://cran.r-project.org/doc/manuals/R-lang.html}{R language
definition}, it is informal and incomplete. The R-language is mostly
defined in terms of how GNU-R works. This is in contrast to other
languages, like \href{http://isocpp.org/std/the-standard}{C++} and
\href{http://www.ecma-international.org/publications/standards/Ecma-262.htm}{javascript},
that make a clear distinction between language and implementation by
laying out formal specifications that describe in minute detail how
every aspect of the language should work. Nevertheless, the distinction
between R-language and GNU-R is still useful: poor performance due to
the language is hard to fix without breaking existing code; fixing poor
performance due to the implementation is easier.

In \protect\hyperlink{language-performance}{Language performance}, I
discuss some of the ways in which the design of the R-language imposes
fundamental constraints on R's speed. In
\protect\hyperlink{implementation-performance}{Implementation
performance}, I discuss why GNU-R is currently far from the theoretical
maximum, and why improvements in performance happen so slowly. While
it's hard to know exactly how much faster a better implementation could
be, a \textgreater{}10x improvement in speed seems achievable. In
\protect\hyperlink{faster-r}{alternative implementations}, I discuss
some of the promising new implementations of R, and describe one
important technique they use to make R code run faster.

Beyond performance limitations due to design and implementation, it has
to be said that a lot of R code is slow simply because it's poorly
written. Few R users have any formal training in programming or software
development. Fewer still write R code for a living. Most people use R to
understand data: it's more important to get an answer quickly than to
develop a system that will work in a wide variety of situations. This
means that it's relatively easy to make most R code much faster, as
we'll see in the following chapters.

Before we examine some of the slower parts of the R-language and GNU-R,
we need to learn a little about benchmarking so that we can give our
intuitions about performance a concrete foundation.

\hypertarget{microbenchmarking}{%
\section{Microbenchmarking}\label{microbenchmarking}}

A microbenchmark is a measurement of the performance of a very small
piece of code, something that might take microseconds (µs) or
nanoseconds (ns) to run. I'm going to use microbenchmarks to demonstrate
the performance of very low-level pieces of R code, which help develop
your intuition for how R works. This intuition, by-and-large, is not
useful for increasing the speed of real code. The observed differences
in microbenchmarks will typically be dominated by higher-order effects
in real code; a deep understanding of subatomic physics is not very
helpful when baking. Don't change the way you code because of these
microbenchmarks. Instead wait until you've read the practical advice in
the following chapters. \index{microbenchmarks}

The best tool for microbenchmarking in R is the
\href{http://cran.r-project.org/web/packages/microbenchmark/}{microbenchmark}
package. It provides very precise timings, making it possible to compare
operations that only take a tiny amount of time. For example, the
following code compares the speed of two ways of computing a square
root.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(microbenchmark)}

\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\KeywordTok{microbenchmark}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(x),}
\NormalTok{  x }\OperatorTok{^}\StringTok{ }\FloatTok{0.5}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: nanoseconds
##     expr    min     lq  mean median     uq    max neval
##  sqrt(x)    604    745  1226    858  1,010 29,800   100
##    x^0.5 10,100 10,200 11433 10,300 10,600 51,500   100
\end{verbatim}

By default, \texttt{microbenchmark()} runs each expression 100 times
(controlled by the \texttt{times} parameter). In the process, it also
randomises the order of the expressions. It summarises the results with
a minimum (\texttt{min}), lower quartile (\texttt{lq}), median, upper
quartile (\texttt{uq}), and maximum (\texttt{max}). Focus on the median,
and use the upper and lower quartiles (\texttt{lq} and \texttt{uq}) to
get a feel for the variability. In this example, you can see that using
the special purpose \texttt{sqrt()} function is faster than the general
exponentiation operator.

As with all microbenchmarks, pay careful attention to the units: here,
each computation takes about 900 ns, 900 billionths of a second. To help
calibrate the impact of a microbenchmark on run time, it's useful to
think about how many times a function needs to run before it takes a
second. If a microbenchmark takes:

\begin{itemize}
\tightlist
\item
  1 ms, then one thousand calls takes a second
\item
  1 µs, then one million calls takes a second
\item
  1 ns, then one billion calls takes a second
\end{itemize}

The \texttt{sqrt()} function takes about 900 ns, or 0.9 µs, to compute
the square root of 100 numbers. That means if you repeated the operation
a million times, it would take 0.9 s. So changing the way you compute
the square root is unlikely to significantly affect real code.

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Instead of using \texttt{microbenchmark()}, you could use the built-in
  function \texttt{system.time()}. But \texttt{system.time()} is much
  less precise, so you'll need to repeat each operation many times with
  a loop, and then divide to find the average time of each operation, as
  in the code below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\FloatTok{1e6}
\KeywordTok{system.time}\NormalTok{(}\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n) }\KeywordTok{sqrt}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\KeywordTok{length}\NormalTok{(n)}
\KeywordTok{system.time}\NormalTok{(}\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n) x }\OperatorTok{^}\StringTok{ }\FloatTok{0.5}\NormalTok{) }\OperatorTok{/}\StringTok{ }\KeywordTok{length}\NormalTok{(n)}
\end{Highlighting}
\end{Shaded}

  How do the estimates from \texttt{system.time()} compare to those from
  \texttt{microbenchmark()}? Why are they different?
\item
  Here are two other ways to compute the square root of a vector. Which
  do you think will be fastest? Which will be slowest? Use
  microbenchmarking to test your answers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{^}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
\KeywordTok{exp}\NormalTok{(}\KeywordTok{log}\NormalTok{(x) }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  Use microbenchmarking to rank the basic arithmetic operators
  (\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, and \texttt{\^{}}) in
  terms of their speed. Visualise the results. Compare the speed of
  arithmetic on integers vs.~doubles.
\item
  You can change the units in which the microbenchmark results are
  expressed with the \texttt{unit} parameter. Use
  \texttt{unit\ =\ "eps"} to show the number of evaluations needed to
  take 1 second. Repeat the benchmarks above with the eps unit. How does
  this change your intuition for performance?
\end{enumerate}

\hypertarget{language-performance}{%
\section{Language performance}\label{language-performance}}

In this section, I'll explore three trade-offs that limit the
performance of the R-language: extreme dynamism, name lookup with
mutable environments, and lazy evaluation of function arguments. I'll
illustrate each trade-off with a microbenchmark, showing how it slows
GNU-R down. I benchmark GNU-R because you can't benchmark the R-language
(it can't run code). This means that the results are only suggestive of
the cost of these design decisions, but are nevertheless useful. I've
picked these three examples to illustrate some of the trade-offs that
are key to language design: the designer must balance speed,
flexibility, and ease of implementation.

If you'd like to learn more about the performance characteristics of the
R-language and how they affect real code, I highly recommend
\href{http://r.cs.purdue.edu/pub/ecoop12.pdf}{``Evaluating the Design of
the R Language''} by Floreal Morandat, Brandon Hill, Leo Osvald, and Jan
Vitek. It uses a powerful methodology that combines a modified R
interpreter and a wide set of code found in the wild.

\hypertarget{extreme-dynamism}{%
\subsection{Extreme dynamism}\label{extreme-dynamism}}

R is an extremely dynamic programming language. Almost anything can be
modified after it is created. To give just a few examples, you can:

\begin{itemize}
\tightlist
\item
  Change the body, arguments, and environment of functions.
\item
  Change the S4 methods for a generic.
\item
  Add new fields to an S3 object, or even change its class.
\item
  Modify objects outside of the local environment with
  \texttt{\textless{}\textless{}-}.
\end{itemize}

Pretty much the only things you can't change are objects in sealed
namespaces, which are created when you load a package.

The advantage of dynamism is that you need minimal upfront planning. You
can change your mind at any time, iterating your way to a solution
without having to start afresh. The disadvantage of dynamism is that
it's difficult to predict exactly what will happen with a given function
call. This is a problem because the easier it is to predict what's going
to happen, the easier it is for an interpreter or compiler to make an
optimisation. (If you'd like more details, Charles Nutter expands on
this idea at
\href{http://blog.headius.com/2013/05/on-languages-vms-optimization-and-way.html}{On
Languages, VMs, Optimization, and the Way of the World}.) If an
interpreter can't predict what's going to happen, it has to consider
many options before it finds the right one. For example, the following
loop is slow in R, because R doesn't know that \texttt{x} is always an
integer. That means R has to look for the right \texttt{+} method (i.e.,
is it adding doubles, or integers?) in every iteration of the loop.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{0L}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\FloatTok{1e6}\NormalTok{) \{}
\NormalTok{  x <-}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The cost of finding the right method is higher for non-primitive
functions. The following microbenchmark illustrates the cost of method
dispatch for S3, S4, and RC. I create a generic and a method for each OO
system, then call the generic and see how long it takes to find and call
the method. I also time how long it takes to call the bare function for
comparison. \index{methods!cost of dispatch}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\OtherTok{NULL}

\NormalTok{s3 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{UseMethod}\NormalTok{(}\StringTok{"s3"}\NormalTok{)}
\NormalTok{s3.integer <-}\StringTok{ }\NormalTok{f}

\NormalTok{A <-}\StringTok{ }\KeywordTok{setClass}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\KeywordTok{representation}\NormalTok{(}\DataTypeTok{a =} \StringTok{"list"}\NormalTok{))}
\KeywordTok{setGeneric}\NormalTok{(}\StringTok{"s4"}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{standardGeneric}\NormalTok{(}\StringTok{"s4"}\NormalTok{))}
\KeywordTok{setMethod}\NormalTok{(s4, }\StringTok{"A"}\NormalTok{, f)}

\NormalTok{B <-}\StringTok{ }\KeywordTok{setRefClass}\NormalTok{(}\StringTok{"B"}\NormalTok{, }\DataTypeTok{methods =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{rc =}\NormalTok{ f))}

\NormalTok{a <-}\StringTok{ }\KeywordTok{A}\NormalTok{()}
\NormalTok{b <-}\StringTok{ }\NormalTok{B}\OperatorTok{$}\KeywordTok{new}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{microbenchmark}\NormalTok{(}
  \DataTypeTok{fun =} \KeywordTok{f}\NormalTok{(),}
  \DataTypeTok{S3 =} \KeywordTok{s3}\NormalTok{(1L),}
  \DataTypeTok{S4 =} \KeywordTok{s4}\NormalTok{(a),}
  \DataTypeTok{RC =}\NormalTok{ b}\OperatorTok{$}\KeywordTok{rc}\NormalTok{()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: nanoseconds
##  expr    min     lq  mean median     uq       max neval
##   fun    178    274   642    335    438    27,300   100
##    S3  1,290  1,660 28696  1,950  2,390 2,620,000   100
##    S4  1,340  1,720  2447  2,140  2,750    15,800   100
##    RC 10,700 11,100 19838 12,000 15,300   613,000   100
\end{verbatim}

The bare function takes about 300 ns. S3 method dispatch takes an
additional 2,000 ns; S4 dispatch, 2,000 ns; and 10,000 dispatch, 10,000
ns. S3 and S4 method dispatch are expensive because R must search for
the right method every time the generic is called; it might have changed
between this call and the last. R could do better by caching methods
between calls, but caching is hard to do correctly and a notorious
source of bugs.

\hypertarget{name-lookup-with-mutable-environments}{%
\subsection{Name lookup with mutable
environments}\label{name-lookup-with-mutable-environments}}

It's surprisingly difficult to find the value associated with a name in
the R-language. This is due to combination of lexical scoping and
extreme dynamism. Take the following example. Each time we print
\texttt{a} it comes from a different environment:
\index{names!cost of lookup}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a <-}\StringTok{ }\DecValTok{1}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{  g <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
    \KeywordTok{print}\NormalTok{(a)}
    \KeywordTok{assign}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DataTypeTok{envir =} \KeywordTok{parent.frame}\NormalTok{())}
    \KeywordTok{print}\NormalTok{(a)}
\NormalTok{    a <-}\StringTok{ }\DecValTok{3}
    \KeywordTok{print}\NormalTok{(a)}
\NormalTok{  \}}
  \KeywordTok{g}\NormalTok{()}
\NormalTok{\}}
\KeywordTok{f}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
## [1] 3
\end{verbatim}

This means that you can't do name lookup just once: you have to start
from scratch each time. This problem is exacerbated by the fact that
almost every operation is a lexically scoped function call. You might
think the following simple function calls two functions: \texttt{+} and
\texttt{\^{}}. In fact, it calls four because \texttt{\{} and \texttt{(}
are regular functions in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{}
\NormalTok{  (x }\OperatorTok{+}\StringTok{ }\NormalTok{y) }\OperatorTok{^}\StringTok{ }\DecValTok{2}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Since these functions are in the global environment, R has to look
through every environment in the search path, which could easily be 10
or 20 environments. The following microbenchmark hints at the
performance costs. We create four versions of \texttt{f()}, each with
one more environment (containing 26 bindings) between the environment of
\texttt{f()} and the base environment where \texttt{+}, \texttt{\^{}},
\texttt{(}, and \texttt{\{} are defined.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random_env <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{parent =} \KeywordTok{globalenv}\NormalTok{()) \{}
\NormalTok{  letter_list <-}\StringTok{ }\KeywordTok{setNames}\NormalTok{(}\KeywordTok{as.list}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{26}\NormalTok{)), LETTERS)}
  \KeywordTok{list2env}\NormalTok{(letter_list, }\DataTypeTok{envir =} \KeywordTok{new.env}\NormalTok{(}\DataTypeTok{parent =}\NormalTok{ parent))}
\NormalTok{\}}
\NormalTok{set_env <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(f, e) \{}
  \KeywordTok{environment}\NormalTok{(f) <-}\StringTok{ }\NormalTok{e}
\NormalTok{  f}
\NormalTok{\}}
\NormalTok{f2 <-}\StringTok{ }\KeywordTok{set_env}\NormalTok{(f, }\KeywordTok{random_env}\NormalTok{())}
\NormalTok{f3 <-}\StringTok{ }\KeywordTok{set_env}\NormalTok{(f, }\KeywordTok{random_env}\NormalTok{(}\KeywordTok{environment}\NormalTok{(f2)))}
\NormalTok{f4 <-}\StringTok{ }\KeywordTok{set_env}\NormalTok{(f, }\KeywordTok{random_env}\NormalTok{(}\KeywordTok{environment}\NormalTok{(f3)))}

\KeywordTok{microbenchmark}\NormalTok{(}
  \KeywordTok{f}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \KeywordTok{f2}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \KeywordTok{f3}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \KeywordTok{f4}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \DataTypeTok{times =} \DecValTok{10000}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: nanoseconds
##      expr min    lq mean median    uq       max neval
##   f(1, 2) 411   585 1215    717   819 3,970,000 10000
##  f2(1, 2) 717   906 1195  1,090 1,220    56,000 10000
##  f3(1, 2) 771   968 1553  1,170 1,310 2,690,000 10000
##  f4(1, 2) 839 1,020 1385  1,230 1,370    81,100 10000
\end{verbatim}

Each additional environment between \texttt{f()} and the base
environment makes the function slower by about 30 ns.

It might be possible to implement a caching system so that R only needs
to look up the value of each name once. This is hard because there are
so many ways to change the value associated with a name:
\texttt{\textless{}\textless{}-}, \texttt{assign()}, \texttt{eval()},
and so on. Any caching system would have to know about these functions
to make sure the cache was correctly invalidated and you didn't get an
out-of-date value. \indexc{<<-}

Another simple fix would be to add more built-in constants that you
can't override. This, for example, would mean that R always knew exactly
what \texttt{+}, \texttt{-}, \texttt{\{}, and \texttt{(} meant, and you
wouldn't have to repeatedly look up their definitions. That would make
the interpreter more complicated (because there are more special cases)
and hence harder to maintain, and the language less flexible. This would
change the R-language, but it would be unlikely to affect much existing
code because it's such a bad idea to override functions like \texttt{\{}
and \texttt{(}.

\hypertarget{lazy-evaluation-overhead}{%
\subsection{Lazy evaluation overhead}\label{lazy-evaluation-overhead}}

In R, function arguments are evaluated lazily (as discussed in
\protect\hyperlink{lazy-evaluation}{lazy evaluation} and
\protect\hyperlink{capturing-expressions}{capturing expressions}). To
implement lazy evaluation, R uses a promise object that contains the
expression needed to compute the result and the environment in which to
perform the computation. Creating these objects has some overhead, so
each additional argument to a function decreases its speed a little.
\index{lazy evaluation!overhead of}

The following microbenchmark compares the runtime of a very simple
function. Each version of the function has one additional argument. This
suggests that adding an additional argument slows the function down by
\textasciitilde{}20 ns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f0 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() }\OtherTok{NULL}
\NormalTok{f1 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{) }\OtherTok{NULL}
\NormalTok{f2 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{, }\DataTypeTok{b =} \DecValTok{1}\NormalTok{) }\OtherTok{NULL}
\NormalTok{f3 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{, }\DataTypeTok{b =} \DecValTok{2}\NormalTok{, }\DataTypeTok{c =} \DecValTok{3}\NormalTok{) }\OtherTok{NULL}
\NormalTok{f4 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{, }\DataTypeTok{b =} \DecValTok{2}\NormalTok{, }\DataTypeTok{c =} \DecValTok{4}\NormalTok{, }\DataTypeTok{d =} \DecValTok{4}\NormalTok{) }\OtherTok{NULL}
\NormalTok{f5 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{, }\DataTypeTok{b =} \DecValTok{2}\NormalTok{, }\DataTypeTok{c =} \DecValTok{4}\NormalTok{, }\DataTypeTok{d =} \DecValTok{4}\NormalTok{, }\DataTypeTok{e =} \DecValTok{5}\NormalTok{) }\OtherTok{NULL}
\KeywordTok{microbenchmark}\NormalTok{(}\KeywordTok{f0}\NormalTok{(), }\KeywordTok{f1}\NormalTok{(), }\KeywordTok{f2}\NormalTok{(), }\KeywordTok{f3}\NormalTok{(), }\KeywordTok{f4}\NormalTok{(), }\KeywordTok{f5}\NormalTok{(), }\DataTypeTok{times =} \DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: nanoseconds
##  expr min  lq mean median  uq       max neval
##  f0() 181 292  475    326 353 1,040,000 10000
##  f1() 199 318  552    347 387 1,410,000 10000
##  f2() 222 355  603    394 445 1,270,000 10000
##  f3() 253 397  672    434 506 1,290,000 10000
##  f4() 282 437  718    481 563 1,100,000 10000
##  f5() 307 470 1124    520 616 3,110,000 10000
\end{verbatim}

In most other programming languages there is little overhead for adding
extra arguments. Many compiled languages will even warn you if arguments
are never used (like in the above example), and automatically remove
them from the function.

\hypertarget{exercises-1}{%
\subsection{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{scan()} has the most arguments (21) of any base function.
  About how much time does it take to make 21 promises each time scan is
  called? Given a simple input (e.g.,
  \texttt{scan(text\ =\ "1\ 2\ 3",\ quiet\ =\ T)}) what proportion of
  the total run time is due to creating those promises?
\item
  Read \href{http://r.cs.purdue.edu/pub/ecoop12.pdf}{``Evaluating the
  Design of the R Language''}. What other aspects of the R-language slow
  it down? Construct microbenchmarks to illustrate.
\item
  How does the performance of S3 method dispatch change with the length
  of the class vector? How does performance of S4 method dispatch change
  with number of superclasses? How about RC?
\item
  What is the cost of multiple inheritance and multiple dispatch on S4
  method dispatch?
\item
  Why is the cost of name lookup less for functions in the base package?
\end{enumerate}

\hypertarget{implementation-performance}{%
\section{Implementation performance}\label{implementation-performance}}

The design of the R language limits its maximum theoretical performance,
but GNU-R is currently nowhere near that maximum. There are many things
that can (and will) be done to improve performance. This section
discusses some aspects of GNU-R that are slow not because of their
definition, but because of their implementation.

R is over 20 years old. It contains nearly 800,000 lines of code (about
45\% C, 19\% R, and 17\% Fortran). Changes to base R can only be made by
members of the R Core Team (or R-core for short). Currently R-core has
\href{http://www.r-project.org/contributors.html}{twenty members}, but
only six are active in day-to-day development. No one on R-core works
full time on R. Most are statistics professors who can only spend a
relatively small amount of their time on R. Because of the care that
must be taken to avoid breaking existing code, R-core tends to be very
conservative about accepting new code. It can be frustrating to see
R-core reject proposals that would improve performance. However, the
overriding concern for R-core is not to make R fast, but to build a
stable platform for data analysis and statistics. \index{R-core}

Below, I'll show two small, but illustrative, examples of parts of R
that are currently slow but could, with some effort, be made faster.
They are not critical parts of base R, but they have been sources of
frustration for me in the past. As with all microbenchmarks, these won't
affect the performance of most code, but can be important for special
cases.

\hypertarget{extracting-a-single-value-from-a-data-frame}{%
\subsection{Extracting a single value from a data
frame}\label{extracting-a-single-value-from-a-data-frame}}

The following microbenchmark shows five ways to access a single value
(the number in the bottom-right corner) from the built-in
\texttt{mtcars} dataset. The variation in performance is startling: the
slowest method takes 30x longer than the fastest. There's no reason that
there has to be such a huge difference in performance. It's simply that
no one has had the time to fix it. \index{subsetting!performance}
\indexc{[}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{microbenchmark}\NormalTok{(}
  \StringTok{"[32, 11]"}\NormalTok{      =}\StringTok{ }\NormalTok{mtcars[}\DecValTok{32}\NormalTok{, }\DecValTok{11}\NormalTok{],}
  \StringTok{"$carb[32]"}\NormalTok{     =}\StringTok{ }\NormalTok{mtcars}\OperatorTok{$}\NormalTok{carb[}\DecValTok{32}\NormalTok{],}
  \StringTok{"[[c(11, 32)]]"}\NormalTok{ =}\StringTok{ }\NormalTok{mtcars[[}\KeywordTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{32}\NormalTok{)]],}
  \StringTok{"[[11]][32]"}\NormalTok{    =}\StringTok{ }\NormalTok{mtcars[[}\DecValTok{11}\NormalTok{]][}\DecValTok{32}\NormalTok{],}
  \StringTok{".subset2"}\NormalTok{      =}\StringTok{ }\KeywordTok{.subset2}\NormalTok{(mtcars, }\DecValTok{11}\NormalTok{)[}\DecValTok{32}\NormalTok{]}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: nanoseconds
##           expr    min     lq  mean median     uq     max neval
##       [32, 11] 19,500 26,000 27557 26,600 27,400  75,100   100
##      $carb[32]  2,520  3,530  3774  3,750  4,010   9,770   100
##  [[c(11, 32)]]  9,670 11,900 12483 12,200 12,700  28,200   100
##     [[11]][32]  9,310 11,200 19201 11,700 12,300 653,000   100
##       .subset2    431    774   869    874    953   3,420   100
\end{verbatim}

\hypertarget{ifelse-pmin-and-pmax}{%
\subsection{\texorpdfstring{\texttt{ifelse()}, \texttt{pmin()}, and
\texttt{pmax()}}{ifelse(), pmin(), and pmax()}}\label{ifelse-pmin-and-pmax}}

Some base functions are known to be slow. For example, take the
following three implementations of \texttt{squish()}, a function that
ensures that the smallest value in a vector is at least \texttt{a} and
its largest value is at most \texttt{b}. The first implementation,
\texttt{squish\_ife()}, uses \texttt{ifelse()}. \texttt{ifelse()} is
known to be slow because it is relatively general and must evaluate all
arguments fully. The second implementation, \texttt{squish\_p()}, uses
\texttt{pmin()} and \texttt{pmax()}. Because these two functions are so
specialised, one might expect that they would be fast. However, they're
actually rather slow. This is because they can take any number of
arguments and they have to do some relatively complicated checks to
determine which method to use. The final implementation uses basic
subassignment. \indexc{ifelse()} \indexc{pmin()} \indexc{pmax()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{squish_ife <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, a, b) \{}
  \KeywordTok{ifelse}\NormalTok{(x }\OperatorTok{<=}\StringTok{ }\NormalTok{a, a, }\KeywordTok{ifelse}\NormalTok{(x }\OperatorTok{>=}\StringTok{ }\NormalTok{b, b, x))}
\NormalTok{\}}
\NormalTok{squish_p <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, a, b) \{}
  \KeywordTok{pmax}\NormalTok{(}\KeywordTok{pmin}\NormalTok{(x, b), a)}
\NormalTok{\}}
\NormalTok{squish_in_place <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, a, b) \{}
\NormalTok{  x[x }\OperatorTok{<=}\StringTok{ }\NormalTok{a] <-}\StringTok{ }\NormalTok{a}
\NormalTok{  x[x }\OperatorTok{>=}\StringTok{ }\NormalTok{b] <-}\StringTok{ }\NormalTok{b}
\NormalTok{  x}
\NormalTok{\}}

\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\FloatTok{-1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{)}
\KeywordTok{microbenchmark}\NormalTok{(}
  \DataTypeTok{squish_ife      =} \KeywordTok{squish_ife}\NormalTok{(x, }\DecValTok{-1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \DataTypeTok{squish_p        =} \KeywordTok{squish_p}\NormalTok{(x, }\DecValTok{-1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \DataTypeTok{squish_in_place =} \KeywordTok{squish_in_place}\NormalTok{(x, }\DecValTok{-1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \DataTypeTok{unit =} \StringTok{"us"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: microseconds
##             expr   min    lq mean median    uq   max neval
##       squish_ife 31.00 35.20 94.2  39.10 41.80 5,440   100
##         squish_p 23.60 28.00 77.0  30.10 32.10 3,680   100
##  squish_in_place  5.04  6.06 64.1   6.73  7.66 5,640   100
\end{verbatim}

Using \texttt{pmin()} and \texttt{pmax()} is about 1x faster than
\texttt{ifelse()}, and using subsetting directly is about 4x as fast
again. We can often do even better by using C++. The following example
compares the best R implementation to a relatively simple, if verbose,
implementation in C++. Even if you've never used C++, you should still
be able to follow the basic strategy: loop over every element in the
vector and perform a different action depending on whether or not the
value is less than \texttt{a} and/or greater than \texttt{b}.

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{#include }\ImportTok{<Rcpp.h>}
\KeywordTok{using} \KeywordTok{namespace}\NormalTok{ Rcpp;}

\CommentTok{// [[Rcpp::export]]}
\NormalTok{NumericVector squish_cpp(NumericVector x, }\DataTypeTok{double}\NormalTok{ a, }\DataTypeTok{double}\NormalTok{ b) \{}
  \DataTypeTok{int}\NormalTok{ n = x.length();}
\NormalTok{  NumericVector out(n);}

  \ControlFlowTok{for}\NormalTok{ (}\DataTypeTok{int}\NormalTok{ i = }\DecValTok{0}\NormalTok{; i < n; ++i) \{}
    \DataTypeTok{double}\NormalTok{ xi = x[i];}
    \ControlFlowTok{if}\NormalTok{ (xi < a) \{}
\NormalTok{      out[i] = a;}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (xi > b) \{}
\NormalTok{      out[i] = b;}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      out[i] = xi;}
\NormalTok{    \}}
\NormalTok{  \}}

  \ControlFlowTok{return}\NormalTok{ out;}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

(You'll learn how to access this C++ code from R in
\protect\hyperlink{rcpp}{Rcpp}.)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{microbenchmark}\NormalTok{(}
  \DataTypeTok{squish_in_place =} \KeywordTok{squish_in_place}\NormalTok{(x, }\DecValTok{-1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \DataTypeTok{squish_cpp      =} \KeywordTok{squish_cpp}\NormalTok{(x, }\DecValTok{-1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \DataTypeTok{unit =} \StringTok{"us"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: microseconds
##             expr  min   lq  mean median   uq   max neval
##  squish_in_place 4.74 5.65  6.68   6.10 6.92    39   100
##       squish_cpp 4.44 5.16 25.91   5.54 6.08 2,010   100
\end{verbatim}

The C++ implementation is around 1x faster than the best pure R
implementation.

\hypertarget{exercises-2}{%
\subsection{Exercises}\label{exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The performance characteristics of \texttt{squish\_ife()},
  \texttt{squish\_p()}, and \texttt{squish\_in\_place()} vary
  considerably with the size of \texttt{x}. Explore the differences.
  Which sizes lead to the biggest and smallest differences?
\item
  Compare the performance costs of extracting an element from a list, a
  column from a matrix, and a column from a data frame. Do the same for
  rows.
\end{enumerate}

\hypertarget{faster-r}{%
\section{Alternative R implementations}\label{faster-r}}

There are some exciting new implementations of R. While they all try to
stick as closely as possible to the existing language definition, they
improve speed by using ideas from modern interpreter design. The four
most mature open-source projects are:
\index{alternative implementations}

\begin{itemize}
\item
  \href{http://www.pqr-project.org/}{pqR} (pretty quick R) by Radford
  Neal. Built on top of R 2.15.0, it fixes many obvious performance
  issues, and provides better memory management and some support for
  automatic multithreading. \index{pqR}
\item
  \href{http://www.renjin.org/}{Renjin} by BeDataDriven. Renjin uses the
  Java virtual machine, and has an extensive
  \href{http://packages.renjin.org/}{test suite}. \index{Renjin}
\item
  \href{https://github.com/allr/fastr}{FastR} by a team from Purdue.
  FastR is similar to Renjin, but it makes more ambitious optimisations
  and is somewhat less mature. \index{FastR}
\item
  \href{https://github.com/jtalbot/riposte}{Riposte} by Justin Talbot
  and Zachary DeVito. Riposte is experimental and ambitious. For the
  parts of R it implements, it is extremely fast. Riposte is described
  in more detail in
  \href{http://www.justintalbot.com/wp-content/uploads/2012/10/pact080talbot.pdf}{Riposte:
  A Trace-Driven Compiler and Parallel VM for Vector Code in R}.
  \index{Riposte}
\end{itemize}

These are roughly ordered from most practical to most ambitious. Another
project, \href{http://www.cs.kent.ac.uk/projects/cxxr/}{CXXR} by Andrew
Runnalls, does not provide any performance improvements. Instead, it
aims to refactor R's internal C code in order to build a stronger
foundation for future development, to keep behaviour identical to GNU-R,
and to create better, more extensible documentation of its internals.
\index{CXXR}

R is a huge language and it's not clear whether any of these approaches
will ever become mainstream. It's a hard task to make an alternative
implementation run all R code in the same way as GNU-R. Can you imagine
having to reimplement every function in base R to be not only faster,
but also to have exactly the same documented bugs? However, even if
these implementations never make a dent in the use of GNU-R, they still
provide benefits:

\begin{itemize}
\item
  Simpler implementations make it easy to validate new approaches before
  porting to GNU-R.
\item
  Knowing which aspects of the language can be changed with minimal
  impact on existing code and maximal impact on performance can help to
  guide us to where we should direct our attention.
\item
  Alternative implementations put pressure on the R-core to incorporate
  performance improvements.
\end{itemize}

One of the most important approaches that pqR, Renjin, FastR, and
Riposte are exploring is the idea of deferred evaluation. As Justin
Talbot, the author of Riposte, points out: ``for long vectors, R's
execution is completely memory bound. It spends almost all of its time
reading and writing vector intermediates to memory''. If we could
eliminate these intermediate vectors, we could improve performance and
reduce memory usage. \index{deferred evaluation}

The following example shows a very simple example of how deferred
evaluation can help. We have three vectors, \texttt{x}, \texttt{y},
\texttt{z}, each containing 1 million elements, and we want to find the
sum of \texttt{x} + \texttt{y} where \texttt{z} is TRUE. (This
represents a simplification of a pretty common sort of data analysis
question.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\FloatTok{1e6}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\FloatTok{1e6}\NormalTok{)}
\NormalTok{z <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(T, F), }\FloatTok{1e6}\NormalTok{, }\DataTypeTok{rep =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{sum}\NormalTok{((x }\OperatorTok{+}\StringTok{ }\NormalTok{y)[z])}
\end{Highlighting}
\end{Shaded}

In R, this creates two big temporary vectors: \texttt{x\ +\ y}, 1
million elements long, and \texttt{(x\ +\ y){[}z{]}}, about 500,000
elements long. This means you need to have extra memory available for
the intermediate calculation, and you have to shuttle the data back and
forth between the CPU and memory. This slows computation down because
the CPU can't work at maximum efficiency if it's always waiting for more
data to come in.

However, if we rewrote the function using a loop in a language like C++,
we only need one intermediate value: the sum of all the values we've
seen:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{#include }\ImportTok{<Rcpp.h>}
\KeywordTok{using} \KeywordTok{namespace}\NormalTok{ Rcpp;}

\CommentTok{// [[Rcpp::export]]}
\DataTypeTok{double}\NormalTok{ cond_sum_cpp(NumericVector x, NumericVector y, }
\NormalTok{                    LogicalVector z) \{}
  \DataTypeTok{double}\NormalTok{ sum = }\DecValTok{0}\NormalTok{;}
  \DataTypeTok{int}\NormalTok{ n = x.length();}

  \ControlFlowTok{for}\NormalTok{(}\DataTypeTok{int}\NormalTok{ i = }\DecValTok{0}\NormalTok{; i < n; i++) \{}
    \ControlFlowTok{if}\NormalTok{ (!z[i]) }\ControlFlowTok{continue}\NormalTok{;}
\NormalTok{    sum += x[i] + y[i];}
\NormalTok{  \}}

  \ControlFlowTok{return}\NormalTok{ sum;}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cond_sum_r <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y, z) \{}
  \KeywordTok{sum}\NormalTok{((x }\OperatorTok{+}\StringTok{ }\NormalTok{y)[z])}
\NormalTok{\}}

\KeywordTok{microbenchmark}\NormalTok{(}
  \DataTypeTok{cond_sum_cpp =} \KeywordTok{cond_sum_cpp}\NormalTok{(x, y, z),}
  \DataTypeTok{cond_sum_r =} \KeywordTok{cond_sum_r}\NormalTok{(x, y, z),}
  \DataTypeTok{unit =} \StringTok{"ms"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: milliseconds
##          expr   min    lq  mean median    uq   max neval
##  cond_sum_cpp  5.27  6.18  6.53   6.44  6.89  8.48   100
##    cond_sum_r 18.30 20.10 23.61  23.70 26.80 32.10   100
\end{verbatim}

On my computer, this approach is about 4x faster than the vectorised R
equivalent, which is already pretty fast.

The goal of deferred evaluation is to perform this transformation
automatically, so you can write concise R code and have it automatically
translated into efficient machine code. Sophisticated translators can
also figure out how to make the most of multiple cores. In the above
example, if you have four cores, you could split \texttt{x}, \texttt{y},
and \texttt{z} into four pieces performing the conditional sum on each
core, then adding together the four individual results. Deferred
evaluation can also work with for loops, automatically discovering
operations that can be vectorised.

This chapter has discussed some of the fundamental reasons that R is
slow. The following chapters will give you the tools to do something
about it when it impacts your code.
