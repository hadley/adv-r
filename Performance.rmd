---
title: Performance
layout: default
---

# Performance

```{r, echo = FALSE}
library(microbenchmark)
options(digits = 3)
options("microbenchmark.unit" = "ns")
```

<!--
Once more complete, circulate to Justin, Radford, Alex & Jan for comments.
-->

As far as computer languages go, R is pretty slow. However, in practice this isn't usually a problem. In data analysis, the bottleneck is usually your brain: you spend more time thinking about what you should compute compared to actually performing the computations. However, as you start to develop more complicated code that works on large datasets, you may find computation becomes a bottleneck. That's because R's design is not optimised for performance, but instead it's optimised for expressiveness.  John Chambers, the designer of the S language that underlies R, recognised that the biggest bottleneck in most data analyses is congitive, not computational. It takes much longer to figure out what you want do, and express it 

The goal of this part of the book is to give you a deeper understanding into R's performance characteristics. You'll learn where R is fast, where it's slow, and how you can improve speed when you need to.

* In this chapter, I'll discuss some of the reasons why R is slow, and help you build a better intuition for what operations are likely to be problematic. I'll also talk a little about the future of R, and what performance problems may improve in the future. Throughout the chapter I'll use microbenchmarking as a tool to quantitatively explore R's performance characterisics. 

* In [Profiling](#profiling), you'll learn concrete tools for making your code faster by first figuring out what you need to optimise and then learning some general tools to optimise it. 

* In [Memory](#memory), you'll learn about how R works with memory, and some common performance problems.

* For really high-performance code, you often need to use another programming language. [Rcpp](#rcpp) will teach you the absolute minimum you need to know about C++ in order to write fast C++ code linked to R through Rcpp.

* Finally, if you want to deeply understand the performance of built in base functions, you'll need to learn a little bit about R's C api. In [R's C interface](#c-api), you'll learn more about the R's C internals and see how some common built-in functions work.

A recurring theme throughout this part of the book is the importance of differentiating between absoluate and relative speed, and fast vs fast enough. First, whenever you compare the speed of two approaches to a problem, be very wary of just looking at a relative differences. One approach may be 10x faster than another, but if that difference is between 1ms and 10ms, it's unlikely to have any real impact. You also need to think about the costs of modifying your code. For example, if it takes you an hour to implement a change that makes you code 10x faster, saving 9 s each run, then you'll have to run at least 400 times before you'll see a net benefit.  At the end of the day, what you want is code that's fast enough to not be a bottleneck, not code that is fast in any absolute sense.  Be careful that you don't spend hours to save seconds.

## Microbenchmarking

A microbenchmark is a performance measurement of a very small piece of code, something that might take microseconds (µs) or nanoseconds (ns) to run. In this chapter, I'll use microbenchmarks to demonstrate the performance of very low-level pieces of R, to help develop your intution for how R works. This intuition, by-and-large, is not practically useful because in most real slow code saving microseconds does not have an appreciable effect on run time. You should not change the way you code because of these microbenchmarks, but instead wait until the next chapter to learn how to speed up real code.

The best tool for microbenchmarking in R is (surprise!) the [microbenchmark][microbenchmark] package, which offers very precise times. This makes it much easier to compare operations that only take a small amount of time. The following code shows a simple example of using `microbenchmark()` to compare two ways of computing a square root:

```{r}
library(microbenchmark)

x <- runif(100)
microbenchmark(
  sqrt(x),
  x ^ 0.5
)
```

Not surprisingly, using a special purpose function is faster than a general purpose function.

My favourite features of the microbenchmark package are:

* The timing of every input expression is repeated 100 times, interleaved in random order. This allows you to see the variability associated with the estimates, while controlling for systematic variability.

* All individual timings are stored in its output, but only summaries are displayed with the default method of `print()`.

* You can see the variability in timings with `boxplot()`, or if if ggplot2 is loaded, `autoplot()` methods.

### Exercises

* Using Compare the microbenchmark to `system.time()`. Why are the estimates of the individual operations different?

    ```{r, eval = FALSE}
    n <- 1:1e6
    system.time(for (i in n) sqrt(x)) / length(n)
    system.time(for (i in n) x ^ 0.5) / length(n)
    ```

* Here are a few other ways to compute the square root of a vector. Which do you think will be fastest? Which will be slowest? Use microbenchmarking to confirm your answers.

    ```{r, eval = FALSE}
    x ^ (1 / 2)
    exp(log(x) / 2)
    ```

## What makes R slow?

In the remainder of this chapter I'll discuss the reasons why R is slow, illustrating each reason with a microbenchmark. By and large R is slow because it is optimised for expressiveness, not speed. This is a tradeoff between writing and running code: R makes it faster for you to write code at the cost of making it harder for the computer to run it. Different languages occupy different positions in the space of expressiveness vs performance, which is one of the reasons why one language isn't appropriate for every task.

Generally, there's a tradeoff between expressiveness, performance and ease of implementation. It's easy to make a language that's fast but limited, or expressive but slow. It's hard to make a language that's both expressive and performant. Over time, it's gotten easier to write new languages (better tools and better understanding) and the tradeoffs are better understood, so very generally speaking, modern languages can be more performant for a given level of expressiveness than older languages.

That's not to say that R can't be made much faster. A ten-fold improvement in speed seems achievable, and in [faster R](#faster-r), I'll discuss some of the promising new implementations of R that show how R might do better in the future.

Individually each of the individual aspects maybe slow a function call down by just a few nanoseconds. But together they add up, and are problematic in scenarios where you want to call functions millions or billions of times, like in simulations (esp agent based and mcmc), or heavily recursive code.

At the end of this chapter you might feel like R is so impossibly slow that you can never achieve anything in it, or when you try and tackle some problems you'll feel parallelised because you know what ever approach you take will be hopelessly inefficient. Just remember that while R might be slow compared to other programming languages, for most tasks is fast enough. All of the timings in the microbenchmarks are in nanoseconds and unless you are calling functions billions of times, these inefficiencies don't matter. R has been carefully designed to save time for data analysts (whose time is valuable), not to save time for computers (whose time is cheap). Also just because R slow as a language does not mean it will be slow for a specific task. Many common data analysis bottlenecks have been rewritten in C or Fortran for better speed.

If you'd like to learn more about the performance characteristics of R, I'd particularly recommend the article [Evaluating the Design of the R Language](https://www.cs.purdue.edu/homes/jv/pubs/ecoop12.pdf) by Floreal Morandat, Brandon Hill, Leo Osvald and Jan Vitek. It discusses a better methodology for understand the performance characteristics of R using a modified R interpreter and a wide set of real code.

### Extreme dynamism

Everything can be modified after the fact. After function has been defined, can change its body, arguments and environment. This makes it much harder for compilers to specialise to a special fast case - it might pick the right function the first time, but if you've changed the function it might now point to the wrong thing. This means that a compiler needs to figure out which funtion to call every time, 

R is a very flexible programming language that exposes many itself in ways that most other programming languages do not. For example, you can use `substitute()` to access the expression that will compute the value of a function argument. Or you can use `<<-` to modify values in a non-local environment. The flexibility of the R language makes it harder to optimise, but it makes it easy to build domain specific languages that allow you to clearly express what you want.

The "type"s of objects in R are very dynamic, and functions have no type declarations associated with them. In statically type compiled language, the compiler can do a lot of work up front to figure out exactly which method should get called for a given set of input arguments. Compilation basically means that this lookup happens once in an initial first pass, instead of every time that the function is called.

```{r, results = 'hide'}
f <- function(x) NULL

s3 <- function(x) UseMethod("s3")
s3.integer <- function(x) NULL

A <- setClass("A", representation(a = "list"))
setGeneric("s4", function(x) standardGeneric("s4"))
setMethod(s4, "A", function(x) NULL)

B <- setRefClass("B")
B$methods(r5 = function(x) NULL)

a <- A()
b <- B$new()
```

```{r}
microbenchmark(
  bare = NULL,
  fun = f(),
  s3 = s3(1L),
  s4 = s4(a),
  r5 = b$r5()
)
```

On my computer, the bare call takes about 40 ns. Wrapping it in a function adds about an extra 200 ns - this is the cost of creating the environment where the function execution happens. S3 method dispatch adds around 3 µs and S4 around 12 µs.

However, it's important to notice the units: microseconds. There are a million microseconds in a second, so it will take hundreds of thousands of calls before the cost of S3 or S4 dispatch appreciable. Most problems don't involve hundreds of thousands of function calls, so it's unlikely to be a bottleneck in practice.This is why microbenchmarks can not be considered in isolation: they must be  carefully considered in the context of your real problem.

Another example of this dynamism is that R has very few built in constants because almost every operation is a lexically scoped function call. For example, this simple function:

```{r}
f <- function(x, y) {
  (x + y) ^ 2
}
```

Has four function calls: `{`, `(`, `+`, `^`. These functions are not hard coded constants in R, and can be overriden by other code. This means that to find their definitions, R has to look through every environment on the search path, which could easily be 10 or 20 environments. It would be possible to change this behaviour for R, and affect very little code (it's a bad idea to override `{` or `(`!), but it would require substantial work by R core.

The following microbenchmark hints at the performance costs. We create four versions of `f()`, each with one more environment between the environment of `f()` and the base environment where `+`, `^`, `(`, and `{` are defined. That means there's one more environment that R has to look through every time.

```{r}
random_env <- function(parent = globalenv()) {
  list2env(setNames(as.list(runif(26)), LETTERS), envir = new.env(parent = parent))
}
set_env <- function(f, e) {
  environment(f) <- e
  f
}
f2 <- set_env(f, random_env())
f3 <- set_env(f, random_env(environment(f2)))
f4 <- set_env(f, random_env(environment(f3)))

microbenchmark(
  f(1, 2),
  f2(1, 2),
  f3(1, 2),
  f4(1, 2), times = 1000
)

```

### Lazy evaluation overhead

As discussed in [lazy evaluation](#lazy-evaluation) and [capturing expressions](#capturing-expressions) function arguments are lazily evaluated. Internally, function arguments are implemented as a promise, an object which contains the expression needed to compute the result, and the environment in which to perform the computation. Creating these objects has some overhead, so every additional argument to an R function slows it down a little.

The following microbenchmark shows that each an additional argument slow function execution by about 20 ns.

```{r promise-cost}
f0 <- function() NULL
f1 <- function(a = 1) NULL
f2 <- function(a = 1, b = 1) NULL
f3 <- function(a = 1, b = 2, c = 3) NULL
f4 <- function(a = 1, b = 2, c = 4, d = 4) NULL
f5 <- function(a = 1, b = 2, c = 4, d = 4, e = 5) NULL
microbenchmark(f0(), f1(), f2(), f3(), f4(), f5(), times = 1000)
```

In most other programming languages there is no overhead for adding extra arguments. Many compiled languages will even warn you if arguments are never used, and automatically remove them from the function.

### Missing values

Missing values are built into the language in a very deep way. This is really important for statistic and data analysis, but comes at a cost. For example, whenever you add together two vectors, `x + y`, R can not just use the lowest level addition operator implemented on your processor. For every value of `x` and `y`, R must check whether the value is missing. This means instead of the simple C loop:

```cpp
for (int i = 0; i < n; ++i) {
  z[i] = x[i] + y[i];
}
```

R must do something more like:

```cpp
for (int i = 0; i < n; ++i) {
  if (is_na(x[i]) || is_na(y[i])) {
    z[i] = NA
  } else {
    z[i] = x[i] + y[i];
  }
}
```

This is harder for compilers to figure out how to convert to very efficient machine code. Additionally, many modern processors have single commands that allow you to add multiple numbers together simultaneously (i.e. the chip itself implements vector operations), and it's much harder for R to take advantage of these sorts of features.  SIMD (single instruction, multiple data) instructions allow a chip to add four pairs of number simultaneously, which can be four times as fast.

## Incidental reasons

As well as fundamental reasons that R is slow, and will always be relatively slow, because of design tradeoffs, there as some incidental reasons why R is slow. These are effectively accidents of history and fixing them only requires the (scarce) time of R core.

One important reason that R is slow is that no member of R core is paid to work on R full-time. They are mostly statistics professors. Not formally trained in software development or software engineering.

It can be frustrating to see what look like obvious performance improvements rejected by R-core. This becomes more obvious when you better understand what R core are optimising for: stability, not speed. 

R is a old, big, C project, and change comes slowly. Most of the time this is good: you want your R code to keep working over time. But it also means that it is very hard to make big changes to the code base.  R has accumulated a lot of technical debt which means its difficult to make improvements. Base R doesn't have a great system of unit tests, which makes it difficult to come back after the fact and rewrite code without worrying that you've changed behaviour.

One illustration of these performance 

```{r}
microbenchmark(
  mtcars["Volvo 142E", "carb"],
  mtcars[32, 11],
  mtcars[[c(11, 32)]],
  mtcars[["carb"]][32],
  mtcars[[11]][32],
  mtcars$carb[32],
  .subset2(mtcars, 11)[32]
)
```


Some base functions are known to be slow. For example, look at the following three implementations of a function to `squish()` a vector to make the sure smallest value is at least `a` and the largest value is at most `b`.  
The first implementation uses `ifelse()` which is known to be slow due to the complicated way it figures out what class the output should have. The second uses `pmin()` and `pmax()` which should be much faster because they're so specialised. But they're actually rather slow because they can take any number of arguments and have to do some relatively complicated checks to determine which method to use. The final implementation uses basic subassignment.

```{r}
squish_ife <- function(x, a, b) {
  ifelse(x <= a, a, ifelse(x >= b, b, x))
}
squish_p <- function(x, a, b) {
  pmax(pmin(x, b), a)
}
squish_in_place <- function(x, a, b) {
  x[x <= a] <- a
  x[x >= b] <- b
  x
}

x <- runif(100, -1.5, 1.5)
microbenchmark(
  squish_ife(x, -1, 1),
  squish_p(x, -1, 1),
  squish_in_place(x, -1, 1),
  unit = "ms"
)
```

Another example is `%in%` - it's not efficient as it could be because it's built on top of `match()` which does more work than is needed. 

```{r}
microbenchmark(
  "x" %in% letters, 
  any("x" == letters),
  unit = "ms"
)
```

Most package developers are not professional software engineers, and again are not paid to work on R packages. This means that most package code is written to solve a pressing problem. The vast majority of R code is poorly written and slow. This sounds bad but it's actually a positive! There's no point in optimising code until it's actually a bottleneck - most R code should be incredibly inefficient because even inefficient code is usually fast enough. If most R code was efficient, it would be a strong signal that R programmers are prematurely optimising, spend timing making their code faster instead of solving real problems. 

### Exercises

* The performance characteristics of `trunc_ife()`, `trunc_p()` and `trunc_in_place()` vary considerably with the size of `x`. Explore the differences. For what sizes of input is the difference biggest? Smallest?

* Compare the performance costs of extract an element from a list, a column from a matrix, and a column from a data frame. What about a row?

* `scan()` has the most arguments (21) of any base function. About how much time does it take to make 21 promises each time scan is called? Given a simple input (e.g. `scan(text = "1 2 3", quiet = T)`) what proportion of the total run time is due to creating those promises?

## How might R become faster? {#faster-r}

These are things that could be fixed in a new implementation.

* [Pretty quick R](http://www.pqr-project.org/). By Radford Neal. Built on top of existing R code base (2.15.0).  Fixes many obvious performance issues. Better memory management.

* [CXXR](http://www.cs.kent.ac.uk/projects/cxxr/). Reimplementation of R into clean C++. Not currently faster, but is much more extensible and might form clean foundation for future work. Better documentation of intenrals.  http://www.cs.kent.ac.uk/projects/cxxr/pubs/WIREs2012_preprint.pdf. Behaviour identical.

* [Renjin](http://www.renjin.org/). JVM. Extensive [test suite](http://packages.renjin.org/). Good discussion at http://www.renjin.org/blog/2013-07-30-deep-dive-vector-pipeliner.html. Along the same lines, but currently less developed in [fastr](https://github.com/allr/fastr).  Written by the same group as traceR paper.

* [Riposte](https://github.com/jtalbot/riposte). Experimental VM. (http://www.justintalbot.com/wp-content/uploads/2012/10/pact080talbot.pdf)

(ordered roughly from most practical to most ambitious). 

These are all interesting experiments with R. It's not clear how many will evolve to be long-term alternatives to R. It's hard task to make sure an alternative backend can run all R code in the same way (or similar enough) to GNU R. The challenge with any rewrite is maintaining compatibility with base R. Can you imagine having to reimplement every function in base R to not only be faster, but to have exactly the same documented bugs? (e.g. `nchar(NA)`) But they provide a lot of information about what R can do better, and put pressure on base R to improve performance.

Deferred evaluation: `x + y * z`. http://radfordneal.wordpress.com/2013/07/24/deferred-evaluation-in-renjin-riposte-and-pqr/. JT: "for long vectors, R’s execution is completely memory bound. It spends almost all of its time reading and writing vector intermediates to memory"

1:n

Parallelisation

Scalars

Better memory management.
