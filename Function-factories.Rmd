# Function factories
\index{function factories}

```{r, include = FALSE}
source("common.R")
```

## Introduction

A function factory is a function that makes functions. Here's a simple example: we use a function factory (`power1()`) to make two child functions (`square()` and `cube()`). You have seen all the components individual before; what's new is that we have a function that returns a function:

```{r}
power1 <- function(exp) {
  force(exp)
  
  function(x) {
    x ^ exp
  }
}

square <- power1(2)
cube <- power1(3)
```

Section \@ref(forcing-evaluation).

`square()` and `cube()` are regular functions so we can call them as usual:

```{r}
square(2)
square(4)

cube(2)
cube(4)
```

Function factories are useful not because they tend to reduce overall complexity,  but because they allow you to partition complexity into a small number of pieces that can be more easily understood.

Of the three main FP tools, function factories are probably the least useful. However, they do come in handy every now and then, and the examples in this chapter will show you when and why. 

### Outline {-}

* Section \@ref(factory-fundamentals) begins the chapter with a description
  of how function factories work, pulling together ideas from scoping and 
  environments.

* Section \@ref(stat-fact) uses function factories to help solve two statistical
  challenge: maximum likelihood estimation and boostrapping. In both examples,
  you'll see how function factories can partition work, in order to do some
  upfront, and save time in the long-run.
  
* Section \@ref(mutable-state) shows how you can use `<<-` with function 
  factories in order to preserve state across function calls. You'll learn 
  a richer approach in [R6], but a function factory can be useful for simple 
  cases, like capturing the number of times a function is called. 

* Section \@ref(numerical-integration) explores numerical integration: 
  starting with simple pieces like midpoint, trapezoid, Simpson, and Boole
  and rules, and showing how they can all be generated with a single
  function factory.

* Section \@ref(functional-factories) shows how you can combine function
  factories with functionals to rapidly generate a family of functions
  from data.

Function factories are an important building block for very useful function operators, which you'll learn about in the next chapter.

### Prerequisites {-}

Function environments.

```{r setup}
library(rlang)
```

## Factory fundamentals
\index{closures|see{functions}}

On the surface, a function factory is quite simple, and doesn't involve any new syntax: you just create a function inside another functon and return it. However, to understand function factories, you need to pull together some of the ideas we've touched on in earlier chapters, but not fully explored.

### Environments

The key idea is that enclosing environment of function produced by the factory is the execution environment of the factory function. Here, the enclosing environment of `square()` and `cube()` are the execution environments of `power1()`. That means when you print a closure, you don't see anything terribly useful:

```{r}
square

cube
```

The body of the function is identical; the difference is the enclosing environment, `environment(square)` and `environment(cube)`, which contain the values bound to `exp`.  We can see that the value of `exp` in the two enclosing environments is different:

```{r}
environment(square)$exp
environment(cube)$exp
```

### Diagram conventions

It's a little easier to see what's going on with a diagram:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/function-factories/power-full.png", dpi = 300)
```

However, there's a lot going on this diagram, so we can simplify things with two conventions:

* Any free floating symbol lives in the global environment.

* Any environment without an explicit parent inherits from the global 
  environment.

Those conventions make it possible to simplify:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/function-factories/power-simple.png", dpi = 300)
```

### Garbage collection

The execution environment is usually ephemeral, but because functions bind their enclosing argument, that reference will keep the execution environment alive until the child function is GC'd.  This property makes function factories work, but requires a little care when you have more complicated code, as it's easy to accidentally capture large variables that you don't actually care about. Fir example, in the following code, `f2()` is large because its environment contains the million element vector `x`, even though it's never actually used:

```{r}
f1 <- function() {
  x <- 1:1e6
  function() 10
}
lobstr::obj_size(f1)

f2 <- f1()
lobstr::obj_size(f2)
```

### Exercises

1.  Base R contains two function factories, `approxfun()` and `ecdf()`. 
    Read their documentation and experiment to figure out what the functions 
    do and what they return.

1.  Create a function `pick()` that takes an index, `i`, as an argument and 
    returns a function with an argument `x` that subsets `x` with `i`.

    ```{r, eval = FALSE}
    lapply(mtcars, pick(5))
    # should do the same as this
    lapply(mtcars, function(x) x[[5]])
    ```

## Statistical factories {#stat-fact}

Our first motivating examples for function factories come from statistics: solving maximum likelihood problems and working with bootstraps. In both cases, you can solve without function factories, but I think they're a good fit to the problems and lead to slightly more elegant solutions.

### Maximum likelihood estimation {#MLE}
\index{maximum likelihood}
\indexc{optimise()}
\indexc{optim()}

The goal of maximum likelihood estimation (MLE) is to find the parameter values for a distribution that make the observed data "most likely". To do MLE, you start with a probability function. Let's work through a simple example, using the Poisson distribution. For example, if we know $\lambda$, we can compute the probability of getting a vector $\mathbf{x}$ of values ($x_1$, $x_2$, ..., $x_n$) by multiplying the Poisson probability function as follows:

\[ P(\lambda, \mathbf{x}) = \prod_{i=1}^{n} \frac{\lambda ^ {x_i} e^{-\lambda}}{x_i!} \]

In statistics, we almost always work with the log of this function. The log is a monotonic transformation which preserves important properties (i.e. the extrema occur in the same place), but has specific advantages:

* The log turns a product into a sum, which is easier to work with.

* Multiplying small numbers yields an even smaller number, which makes floating point math more challenging. Logged values are more numerically stable.

Let's log transform this probability function and simplify it as much as possible:

\[ \log(P(\lambda, \mathbf{x})) = \sum_{i=1}^{n} \log(\frac{\lambda ^ {x_i} e^{-\lambda}}{x_i!}) \]

\[ \log(P(\lambda, \mathbf{x})) = \sum_{i=1}^{n} \left( x_i \log(\lambda) - \lambda - \log(x_i!) \right) \]

\[ \log(P(\lambda, \mathbf{x})) = 
     \sum_{i=1}^{n} x_i \log(\lambda)
   - \sum_{i=1}^{n} \lambda 
   - \sum_{i=1}^{n} \log(x_i!) \]

\[ \log(P(\lambda, \mathbf{x})) = 
   \log(\lambda) \sum_{i=1}^{n} x_i - n \lambda - \sum_{i=1}^{n} \log(x_i!) \]

We can now turn this function into an R function. The R function is quite elegant because R is vectorised and, because it's a statistical programming language, R comes with built-in functions like the log-factorial (`lfactorial()`).

```{r}
lprob_poisson <- function(lambda, x) {
  n <- length(x)
  log(lambda) * sum(x) - n * lambda - sum(lfactorial(x))
}
```

Consider this vector of observations:

```{r}
x1 <- c(41, 30, 31, 38, 29, 24, 30, 29, 31, 38)
```

We can use `lprob_poisson()` to compute the (logged) probability of `x1` for different values of `lambda`. 

```{r}
lprob_poisson(10, x1)
lprob_poisson(20, x1)
lprob_poisson(30, x1)
```

The key idea of maximum likelihood is to think about the probability function in a different way. So far we've been thinking of `lambda` as fixed and known and the function tells us the probability of getting different `x` values. But in real-life, we observe the `x` and it is `lambda` that is unknown. The likelihood is the probability function, seen through this lens: we want to find the `lambda` that makes the observed `x` the "most likely". That is, given `x`, what value of `lambda` gives us the highest value of `lprob_poisson()`? 

In R, we can make this change in perspective more clear by using a function factory. We provide `x` and generate a function with a single parameter, `lambda`:

```{r}
ll_poisson <- function(x) {
  n <- length(x)

  function(lambda) {
    log(lambda) * sum(x) - n * lambda - sum(lfactorial(x))
  }
}
```

One nice thing about this approach is we can do some precomputation: any term that only involves `x` can be computed once in the factory. This is useful, because we're going to need to call this function many times to find the best `lambda`.

```{r}
ll_poisson <- function(x) {
  n <- length(x)
  sum_x <- sum(x)
  c <- sum(lfactorial(x))

  function(lambda) {
    log(lambda) * sum_x - n * lambda - c
  }
}
```

Now we can use this function to find the value of `lambda` that maximizes the (log) likelihood:

```{r}
ll1 <- ll_poisson(x1)

ll1(10)
ll1(20)
ll1(30)
```

Rather than trial and error, we can automate the process of finding the best value with `optimise()`. The results tell us that the highest value is `-30.27` which occurs when `lambda = 32.1`:

```{r}
optimise(ll1, c(0, 100), maximum = TRUE)
```

Now, we could have solved this problem without using a function factory because `optimise()` passes `...` on to the function being optimised. That means we could use the log-probability function directly:

```{r}
optimise(lprob_poisson, c(0, 100), x = x1, maximum = TRUE)$maximum
```

The advantage of using a function factory here is fairly small, but there are two niceties:

* We can precompute some values in the factory itself, saving computation time
  in each iteration.
  
* I think the two-level design better reflects the mathematical structure of 
  the underlying problem.

These advantages get bigger in more complex MLE problems, where you have multiple parameters and multiple data vectors.

### Bootstrap generators

Another statistical application of function factories is bootstrapping. Function factories are useful because rather than thinking about a single bootstrap (as you always need more than one), you than think about a bootstrap __generator__, a function that yields a fresh boostrap every time it is called:

```{r}
boot_permute <- function(df, var) {
  n <- nrow(df)
  
  function() {
    df[[var]][sample(n, n, replace = TRUE)]
  }
}

boot_mtcars1 <- boot_permute(mtcars, "mpg")
head(boot_mtcars1())
head(boot_mtcars1())
```

The advantage of a function factory is more clear with a parametric bootstrap where we have to first fit a model. We can do this setup step once, when the factory is called, rather than once every time we generate the bootstrap.

```{r}
boot_model <- function(df, formula) {
  mod <- lm(formula, data = df)
  fitted <- unname(fitted(mod))
  resid <- unname(resid(mod))
  rm(mod)

  function() {
    fitted + sample(resid)
  }
} 

boot_mtcars2 <- boot_model(mtcars, mpg ~ wt)
head(boot_mtcars2())
head(boot_mtcars2())
```

I include `rm(mod)` because base linear model objects are quite large because they include complete copies of the model matrix, and the original data frame.

### Exercises

1.  What does the following statistical function do? What would be a better 
    name for it? (The existing name is a bit of a hint.)

    ```{r}
    bc <- function(lambda) {
      if (lambda == 0) {
        function(x) log(x)
      } else {
        function(x) (x ^ lambda - 1) / lambda
      }
    }
    ```

1.  Create a function that creates functions that compute the ith 
    [central moment](http://en.wikipedia.org/wiki/Central_moment) of a numeric 
    vector. You can test it by running the following code:

    ```{r, eval = FALSE}
    m1 <- moment(1)
    m2 <- moment(2)

    x <- runif(100)
    stopifnot(all.equal(m1(x), 0))
    stopifnot(all.equal(m2(x), var(x) * 99 / 100))
    ```

1.  Why don't you need to worry that `boot_permute()` stores a copy of the 
    data inside the function that it generates?

1.  Things are slightly less elegant when we generalise to more parameters 
    because `optim()`, the n-d generalisation of `optimise()`, calls the 
    function with a single argument containing a vector of parameters.

    ```{r}
    nll_normal <- function(x) {
      n <- length(x)
      
      function(params) {
        mu <- params[[1]]
        sigma <- params[[2]]
        
        n * log(sigma) + sum((x - mu) ^ 2) / (2 * sigma ^ 2)
      }
    }
    
    x3 <- c(10.1, 6.12, 8.48, 6.07, 5.27, 5.06, 6.51, 4.34, 3.68, 5.48)
    nll3 <- nll_normal(x1)
    optim(c(0, 1), nll3)$par
    ```

## Mutable state with `<<-` {#mutable-state}
\indexc{<<-} 
\index{copy-on-modify!exceptions}

Having variables at two levels allows you to maintain state across function invocations. This is possible because while the execution environment is refreshed every time, the enclosing environment is constant. The key to managing variables at different levels is the double arrow assignment operator (`<<-`). Unlike the usual single arrow assignment (`<-`) that always assigns in the current environment, the double arrow operator will keep looking up the chain of parent environments until it finds a matching name. (Section \@label(binding) has more details on how it works.)

Together, a static parent environment and `<<-` make it possible to maintain state across function calls. The following example shows a counter that records how many times a function has been called. Each time `new_counter` is run, it creates an environment, initialises the counter `i` in this environment, and then creates a new function.

```{r}
new_counter <- function() {
  i <- 0
  function() {
    i <<- i + 1
    i
  }
}

counter_one <- new_counter()
counter_two <- new_counter()
```

The enclosing environments of `counter_one()` and `counter_two()` are indepedent because they are the execution environments of `new_counter()` (which is different each time it's called). Ordinarily, function execution environments are temporary, but a closure maintains access to the environment in which it was created. In the example below, closures `counter_one()` and `counter_two()` each get their own enclosing environments when run, so they can maintain different counts.

```{r}
counter_one()
counter_one()
counter_two()
```

The counters get around the "fresh start" limitation by not modifying variables in their execution environment, but in their enclosing environment.

Modifying values in a parent environment is an important technique because it is one way to generate "mutable state" in R. Mutable state is normally hard because every time it looks like you're modifying an object, you're actually creating and then modifying a copy. However, if you do need mutable objects and your code is not very simple, it's usually better to use R6, the topic of Chapter \@ref(R6).

### Exercises

1.  What happens if you don't use a closure? Make predictions then verify with 
    the code below.

    ```{r}
    i <- 0
    new_counter2 <- function() {
      i <<- i + 1
      i
    }
    ```

1.  What happens if you use `<-` instead of `<<-`? Make predictions then verify 
    with the code below.

    ```{r}
    new_counter3 <- function() {
      i <- 0
      function() {
        i <- i + 1
        i
      }
    }
    ```

## Numerical integration {#numerical-integration}
\index{integration}

A powerful use case for functionals is when you have a family of functions with flexible parameters, and some of the members of the family have special, known, names. You can use the function factory to provide a general builder, and then use the factory to give interesting special cases names.

The idea behind numerical integration is simple: find the area under a curve by approximating the curve with simpler components. The two simplest approaches are the __midpoint__ and __trapezoid__ rules. The midpoint rule approximates a curve with a rectangle. The trapezoid rule uses a trapezoid. Each takes the function we want to integrate, `f`, and a range of values, from `a` to `b`, to integrate over. 

For this example, I'll try to integrate `sin x` from 0 to $\pi$. This is a good choice for testing because it has a simple answer: 2.

```{r}
midpoint <- function(f, a, b) {
  (b - a) * f((a + b) / 2)
}

trapezoid <- function(f, a, b) {
  (b - a) / 2 * (f(a) + f(b))
}

midpoint(sin, 0, pi)
trapezoid(sin, 0, pi)
```

Neither of these functions gives a very good approximation. To make them more accurate using the idea that underlies calculus: we'll break up the range into smaller pieces and integrate each piece using one of the simple rules. This is called __composite integration__. I'll implement it using two new functions:

```{r, mid-trap}
midpoint_composite <- function(f, a, b, n = 10) {
  points <- seq(a, b, length = n + 1)
  h <- (b - a) / n

  area <- 0
  for (i in seq_len(n)) {
    area <- area + h * f((points[i] + points[i + 1]) / 2)
  }
  area
}

trapezoid_composite <- function(f, a, b, n = 10) {
  points <- seq(a, b, length = n + 1)
  h <- (b - a) / n

  area <- 0
  for (i in seq_len(n)) {
    area <- area + h / 2 * (f(points[i]) + f(points[i + 1]))
  }
  area
}

midpoint_composite(sin, 0, pi, n = 10)
midpoint_composite(sin, 0, pi, n = 100)
trapezoid_composite(sin, 0, pi, n = 10)
trapezoid_composite(sin, 0, pi, n = 100)
```

```{r, echo = FALSE, eval = FALSE}
mid <- sapply(1:20, function(n) midpoint_composite(sin, 0, pi, n))
trap <- sapply(1:20, function(n) trapezoid_composite(sin, 0, pi, n))
matplot(cbind(mid, trap), 
  xlab = "Number of pieces", ylab = "Estimate of area")
```

You'll notice that there's a lot of duplication between `midpoint_composite()` and `trapezoid_composite()`. Apart from the internal rule used to integrate over a range, they are basically the same. From these specific functions you can extract a more general composite integration function:

```{r}
composite <- function(f, a, b, n = 10, rule) {
  points <- seq(a, b, length = n + 1)

  area <- 0
  for (i in seq_len(n)) {
    area <- area + rule(f, points[i], points[i + 1])
  }

  area
}

composite(sin, 0, pi, n = 10, rule = midpoint)
composite(sin, 0, pi, n = 10, rule = trapezoid)
```

This function takes two functions as arguments: the function to integrate and the integration rule. We can now add even better rules for integrating over smaller ranges:

```{r}
simpson <- function(f, a, b) {
  (b - a) / 6 * (f(a) + 4 * f((a + b) / 2) + f(b))
}

boole <- function(f, a, b) {
  pos <- function(i) a + i * (b - a) / 4
  fi <- function(i) f(pos(i))

  (b - a) / 90 *
    (7 * fi(0) + 32 * fi(1) + 12 * fi(2) + 32 * fi(3) + 7 * fi(4))
}

composite(sin, 0, pi, n = 10, rule = simpson)
composite(sin, 0, pi, n = 10, rule = boole)
```

It turns out that the midpoint, trapezoid, Simpson, and Boole rules are all examples of a more general family called [Newton-Cotes rules](http://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas). (They are polynomials of increasing complexity.) We  can use this common structure to write a function factory that can generate any general Newton-Cotes rule:

```{r}
newton_cotes <- function(coef, open = FALSE) {
  degree <- length(coef) + 2 * open - 1

  function(f, a, b) {
    pos <- function(i) a + i * (b - a) / degree
    points <- pos(seq.int(open, degree - open))

    (b - a) / sum(coef) * sum(f(points) * coef)
  }
}

boole <- newton_cotes(c(7, 32, 12, 32, 7))
milne <- newton_cotes(c(2, -1, 2), open = TRUE)
composite(sin, 0, pi, n = 10, rule = milne)
```

Mathematically, the next step in improving numerical integration is to move from a grid of evenly spaced points to a grid where the points are closer together near the end of the range, such as Gaussian quadrature. That's beyond the scope of this case study, but you could implement it with similar techniques.

### Exercises

1.  The trade-off between integration rules is that more complex rules are 
    slower to compute, but need fewer pieces. For `sin()` in the range 
    [0, $\pi$], determine the number of pieces needed so that each rule will 
    be equally accurate. Illustrate your results with a graph. How do they
    change for different functions? `sin(1 / x^2)` is particularly challenging.

## Function factories + functionals {#functional-factories}

Easily create a bunch of functions at once.

```{r}
names <- list(
  square = 2, 
  cube = 3, 
  root = 1/2, 
  cuberoot = 1/3, 
  reciprocal = -1
)
funs <- purrr::map(names, power1)
funs$root(64)

funs$root
```

If the functional has two arguments, you could use `map2()`, and if it has 3 or more, you could use a data frame plus `pmap()`.

See alternative approach in translation - uses quasiquotation so requires more knowledge, but has the advantage of generating functions with more readable bodies, and avoids accidentally capturing large objects in the enclosing scope. The following code is a quick preview of how we could rewrite `power1()` to use quasiquotation instead of a function factory. You'll learn more about this in Section \@ref(quasi-function).

```{r}
power3 <- function(exponent) {
  new_function(exprs(x = ), expr({
    x ^ !!exponent
  }), env = caller_env())
}

funs <- purrr::map(names, power3)
funs$root(64)
funs$root
```

### Exercises

1.  Instead of creating individual functions (e.g., `midpoint()`, 
    `trapezoid()`, `simpson()`, etc.), we could store them in a list. If we 
    did that, how would that change the code? Can you create the list of 
    functions from a list of coefficients for the Newton-Cotes formulae?
