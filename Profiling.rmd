---
title: Profiling and benchmarking
layout: default
---

```{r, echo = FALSE}
source("code/microbenchmark.r")
```

# Profiling and performance optimisation {#profiling}

> "We should forget about small efficiencies, say about 97% of the time:
> premature optimization is the root of all evil" --- Donald Knuth.

The key idea in this chapter can be summed up simply: "Find out what's then make it fast".  The first part of this chapter introduces you to tools to help understand what makes your code slow. The second part introduces you to some general tools for making your code faster.

Unfortunately optimisations are typically tightly coupled with the problem. It's hard to give give advice that will work in every situation, but I'll do my best. I include some general techniques, and so

Explore

Your code should be correct, maintainable and fast. Notice that speed comes last - if your function is incorrect or unmaintainable (i.e. will eventually become incorrect) it doesn't matter if it's fast. As computers get faster and R is optimised, your code will get faster all by itself. Your code is never going to automatically become correct or elegant if it is not already.

When making code faster be careful not to make it incorrect.


Like javascript, the vast majority of R code is poorly written and slow. This sounds bad but it's actually a positive! There's no point in optimising code until it's actually a bottleneck - most R code should be incredibly inefficient because even inefficient code is usually fast enough. If most R code was efficient, it would be a strong signal that R programmers are prematurely optimising, spend time making their code faster instead of solving real problems. Additionally, most people writing R code are not programmers. Many of them don't have any formal training in programming or computer science, but are using R because it helps them solve their data analysis problems.

This means that the vast majority of R code can be re-written in R to be more efficient. This often means vectorising code, or avoiding some of the most obvious traps discussed in the [R inferno] (http://www.burns-stat.com/documents/books/the-r-inferno/). There are also other strategies like caching/memoisation that trade space for time. Otherwise, a basic knowledge of data structures and algorithms can help come up with alternative strategies.

This applies not only to packages, but also to base R code. The focus on R code has been making a useful tool, not a blazingly fast programming language. There is huge room for improvement, and base R is only going to get faster over time.

That said, sometimes there are times where you need to make your code faster: spending several hours of your day might save days of computing time for others. The aim of this chapter is to give you the skills to figure out why your code is slow, what you can do to improve it, and ensure that you don't accidentally make it slow again in the future.  You may already be familiar with `system.time`, which tells you how long a block of code takes to run. This is a useful building block, but is a crude tool.

Along the way, you'll also learn about the most common causes of poor performance in R, and how to address them. Sometimes there's no way to improve performance within R, and you'll need to use C++, the topic of [Rcpp](#rcpp).

Having a good test suite is important when tuning the performance of your code: you don't want to make your code fast at the expense of making it incorrect. We won't discuss testing any further in this chapter, but we strongly recommend having a good set of test cases written before you begin optimisation.

Additionally, most people writing R code are not programmers. Many of don't have any formal training in programming or computer science, but are using R because it helps them solve their data analysis problems. This means that the vast majority of R code can be re-written in R to be more efficient. This often means vectorising code, or avoiding some of the most obvious traps discussed later in this chapter. There are also other strategies like caching/memoisation that trade space for time. Otherwise, a basic knowledge of data structures and algorithms can help come up with alternative strategies.

[Mature optimisation](http://carlos.bueno.org/optimization/mature-optimization.pdf) (PDF)

A recurring theme throughout this part of the book is the importance of differentiating between absolute and relative speed, and fast vs fast enough. First, whenever you compare the speed of two approaches to a problem, be very wary of just looking at a relative differences. One approach may be 10x faster than another, but if that difference is between 1ms and 10ms, it's unlikely to have any real impact. You also need to think about the costs of modifying your code. For example, if it takes you an hour to implement a change that makes you code 10x faster, saving 9 s each run, then you'll have to run at least 400 times before you'll see a net benefit.  At the end of the day, what you want is code that's fast enough to not be a bottleneck, not code that is fast in any absolute sense.  Be careful that you don't spend hours to save seconds.

##### Prereqs

`install_github("hadley/lineprof")`

## Understanding performance

R provides a built in tool for profiling: `Rprof`. When active, this records the current call stack to disk every `interval` seconds. This provides a fine grained report showing how long each function takes. The function `summaryRprof` provides a way to turn this list of call stacks into useful information. But I don't think it's terribly useful, because it makes it hard to see the entire structure of the program at once. Instead, we'll use the `profr` package, which turns the call stack into a data.frame that is easier to manipulate and visualise.

Example showing how to use profr.

Sample pictures.

Other tools:
* https://github.com/ltierney/Rpkg-proftools - show flamegraph and call graph

## Improving performance

* use better tools
* alternative algorithms
* trade space for time
* write in C++

Again, remember that unless performance is important for the particular case you should always default to the code that is easier to understand. Use built-in functions that you know are slow if they more clearly express intent. Don't use more-performant but less-clear alternatives until you know they're helping with a bottleneck.

Most important step is to brainstorm as many possible alternative approaches.

Writing fast R code is part of a general task of becoming a better R programming. Reading this book has hopefully helped, but you'll generally need to broaden your education.

Good to have a variety of approaches to call upon.

* [Read blogs](http://www.r-bloggers.com/)

* Take algorithm and data structure course, e.g. https://www.coursera.org/course/algs4partI

* Read other R programming books, like
  [The Art of R Programming](http://amzn.com/1593273843)

* Read other people's R codes

## Use better tools


### Beware of modify in place

One of the most pernicious causes of slow code is inadvertently modifying an object in a loop in such a way that every modification requires the complete object to be copied. Sometimes this happens because R isn't always very good at picking up in place modifications, for example, if you modified a single element in a data frame the entire data frame is copied. Other times, it's because you have thought through the implications:

* every time you add a new element to a vector with `c()` or `append()` the
  entire vector must be copied

* every time you add on to an existing matrix with `cbind()` or `rbind()`
  the entire matrix must be copied

* every time you make a longer string with `paste()` the complete string
  must be copied.

Here's a little benchmark that illustrates the difference. We first generate some random strings, and then combine them either iteratively with a loop with `collapse()`, or once with `paste()`. Note that the peformance of `collapse()` get relatively worse as the number of strings grows: combining 100 strings takes almost 30 times longer than combining 10 strings.

```{r}
random_string <- function() {
  paste(sample(letters, 50, replace = TRUE), collapse = "")
}
strings10 <- replicate(10, random_string())
strings100 <- replicate(100, random_string())

collapse <- function(xs) {
  out <- ""
  for (x in xs) {
    out <- paste0(out, x)
  }
  out
}

microbenchmark(
  collapse(strings10),
  collapse(strings100),
  paste(strings10, collapse = ""),
  paste(strings100, collapse = "")
)
```

[Modification in place]{#modification-in-place} shows some other more subtle examples of this phenomena, and gives you tools to determine whether an object is indeed being modified in place, or is being copied multiple times.

This is Circle 2 in the [R inferno](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf). More examples at https://gist.github.com/wch/7107695.

### Avoid slow functions

As you work more on speeding up your slow R code, you'll start to build up a personal list of slow base functions that are best avoided. Three functions that are on my personal list are:

* If you want to see if a vector contains a single value, `any(x == 10)`
  is much faster than `10 %in% x` or `is.element(10, x).

* `ifelse()` is slow, particularly if you are testing multiple conditions.
  One trick to get around this is to assign a unique number to each combination
  of logical values by using binary. If `x`, `y` and `z` are logical variables
  then `x + 2 * y + 4 * z` will assume a unique number between 1 and 8 to each
  combination of TRUE and FALSE values. See http://rpubs.com/wch/8107 for other
  approaches.

* Writing to a `textConnection()` is very slow, use a temporary file on disk
  instead. (More details at http://rpubs.com/wch/13698).

You'll also learn about a lot of others that you can make faster in the course of working through [Rcpp](#Rcpp).

### Do as little as possible

In many cases, a specific function will be faster than a general function. If you can find a more specific function, use. Otherwise you might need to write it yourself.

* `vapply()` is faster than `sapply()`

* Avoid method dispatch. For S4, you can use `findMethod()` to find the
  method once, and then call that repeatedly.

* Or avoid overhead by calling `.Internal()` functions directly

```{r}
x <- runif(1e2)

microbenchmark(
  mean(x),
  mean.default(x),
  .Internal(mean(x))
)
```

But note that if we make the input a hundred times bigger, the difference barely matters. This is why you shouldn't optimise unless you've correctly identified the code. You may save hardly any time at the cost of introducing bugs.

```{r}
x <- runif(1e4)

microbenchmark(
  mean(x),
  mean.default(x),
  .Internal(mean(x))
)
```


If a bottleneck is a base R function, you can sometimes make it faster by making it more specific. Maybe it computes things that you don't need and are discarding anyway. Maybe it does a lot of error checking that you don't need.

For example, if you have a named list with vectors of equal lengths, you can turn it into a data frame very efficiently. We can make a version of `as.data.frame()` for lists that's around 20x faster, by omitting all checking code. Writing this sort of code typically requires carefully re-writing of the source code of base R functions, remove unused functionality while preserve functionality.

```{r}
quickdf <- function(l) {
  class(l) <- "data.frame"
  attr(l, "row.names") <- .set_row_names(length(l[[1]]))
  l
}

l <- lapply(1:26, function(i) runif(1e3))
names(l) <- letters

microbenchmark(
  quickdf(l),
  as.data.frame(l)
)
```

In other cases, doing as little as possible, means work with the simplest possible object. Data frames are particularly expensive because modifying any value in a data frame will copy the entire data frame. Lists and matrices don't suffer from this problem so can be much faster.

Instead of splitting a data frame, split a vector of indices and then use to index into the data frame when needed.

### Other people's code

One of the easiest ways to speed up your code is to find someone who's already done it! Good idea to search for CRAN packages.

Packages that [use Rcpp](http://cran.r-project.org/web/packages/Rcpp/index.html) are a good place to start, because it's likely that the bottlenecks have been reimplemented in high-performance C++ code.

Become familiar with the [CRAN task view](http://cran.rstudio.com/web/views/) most closely related to your domain. That's a good place to look for new packages.

Stackoverflow can be a useful place to ask.

### Vectorisation

Vectorisation is a powerful tool. Doesn't mean using `apply()` or `lapply()` or even `Vectorise()`. Those just change the interface of the function without changing the performance. A vectorised function takes both vectors as inputs and does the loop in C. You'll learn how to write your own vectorised functions in [Rcpp](#rcpp). But taking advantage of already vectorised functions in R is also really important.

* `apply()` is always slower than a specialised functions: be aware of
  `rowSums()`, `colSums()`, `rowMeans()`, and `colMeans()`.

* Be aware of the most efficient ways to convert continuous to categorical
  values (`findInterval()`) and to re-name categorical values (character
  subsetting).

* Matrix multiplication is very fast, so if you can figure out how to
  frame your problem as matrix math, you may be able to get big speed
  wins.

* Whole object subsetting. `x[is.na(x)] <- 0` will replace all missing
  values in `x` with 0 if `x` is a vector, matrix or data frame.

Not about eliminating for-loops, about thinking about the problem in a whole-object, vectorised way.

But it's not always possible to apply it directly, and you may need to understand the underlying method. The following case study explores how to make many t-tests faster, following "Computing thousands of test statistics simultaneously in R" by Holger Schwender and Tina MÃ¼ller in http://stat-computing.org/newsletter/issues/scgn-18-1.pdf - read the paper to see this method applied to other tests.

Here we have a matrix with 50 columns and 1000 rows.

```{r}
m <- 1000
n <- 50
X <- matrix(rnorm(m * n, 10, 3), m)
cl <- rep(0:1, e = n / 2)
```

There are two basic ways to use `t.test()` with a formula or with a vector of class labels

```{r, cache = TRUE}
system.time(for(i in 1:m) t.test(X[i, ] ~ cl)$stat)
system.time(for(i in 1:m) t.test(X[i, cl == 0], X[i, cl == 1])$stat)
```

Of course, a for loop just computes, but doesn't save, so we might want to use `apply()` instead. This adds little overhead:

```{r}
compT <- function(x, cl){
  t.test(x[cl == 0], x[cl == 1])$stat
}
system.time(apply(X, 1, compT, cl = cl))
```

How can we make this faster? But `stats:::t.test.default` does a lot more than just computing the t-statistic - it also computes the p-value and creates a nice output for printing. Maybe we can make our code faster by stripping out those pieces.

```{r}
my_t <- function(x, cl) {
  t_stat <- function(x) {
    m <- mean(x)
    length <- length(x)
    var <- sum((x - m) ^ 2) / (n - 1)

    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(x[cl == 0])
  g2 <- t_stat(x[cl == 1])

  pooled_se <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / pooled_se
}
system.time(apply(X, 1, my_t, cl = cl))
```

Now that we have a fairly simple function, we can make it faster still by vectorising it. Instead of looping over the array outside the funtion, we do it inside by vectorising `t_stat()` so that it works with a matrix of values.

```{r}
rowtstat <- function(X, cl){
  t_stat <- function(X) {
    m <- rowMeans(X)
    n <- ncol(X)
    var <- rowSums((X - m) ^ 2) / (n - 1)

    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(X[, cl == 0])
  g2 <- t_stat(X[, cl == 1])

  pooled_se <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / pooled_se
}
system.time(rowtstat(X, cl))
```

### Byte code compilation

R 2.13.0 introduced a new byte code compiler which can increase the speed of certain types of code 4-5 fold. This improvement is likely to get better in the future as the compiler implements more optimisations - this is an active area of research.

```{r}
library(compiler)
```

Using the compiler is an easy way to get speed ups - it's easy to use, and if it doesn't work well for your function, then you haven't invested a lot of time in it, and so you haven't lost much. The following example shows the pure R version of `lapply()` from [functionals](#lapply). Compiling it gives a considerable speedup, although it's still not quite as fast as the C version provided by base R.

```{r}
lapply2 <- function(x, f, ...) {
  out <- vector("list", length(x))
  for (i in seq_along(x)) {
    out[[i]] <- f(x[[i]], ...)
  }
  out
}

lapply2_c <- cmpfun(lapply2)

x <- list(1:10, letters, c(F, T), NULL)
microbenchmark(
  lapply2(x, is.null),
  lapply2_c(x, is.null),
  lapply(x, is.null)
)
```

This is a relatively good example for byte code compiling. In most cases you're more like to get a 10-40% speedup. This example optimises well because it uses a for-loop, something that is generally rare in R.

All base R functions are byte code compiled by default.

## Trade space for time

A very general optimisation technique is to trade space for time by caching results. Instead of computing repeatedly, you compute once and then look up again in the future. A special case of caching is memoisation.

http://en.wikipedia.org/wiki/Dynamic_programming: "The idea behind dynamic programming is quite simple. In general, to solve a given problem, we need to solve different parts of the problem (subproblems), then combine the solutions of the subproblems to reach an overall solution. Often when using a more naive method, many of the subproblems are generated and solved many times. The dynamic programming approach seeks to solve each subproblem only once, thus reducing the number of computations: once the solution to a given subproblem has been computed, it is stored or "memo-ized": the next time the same solution is needed, it is simply looked up. This approach is especially useful when the number of repeating subproblems grows exponentially as a function of the size of the input."

`readRDS`, `saveRDS`

Caching packages: memoise, hash, http://cran.r-project.org/web/packages/cacher/index.html, http://cran.r-project.org/web/packages/R.cache/index.html
