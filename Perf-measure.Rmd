# Measuring performance {#perf-measure}
\index{profiling} 
\index{performance!measuring}

```{r include = FALSE}
source("common.R")

num <- function(x) format(round(x), big.mark = ",", scientific = FALSE)
ns <- function(x) paste0(num(round(unclass(x) * 1e9, -1)), " ns")
```

## Introduction

> "Programmers waste enormous amounts of time thinking about, or worrying 
> about, the speed of noncritical parts of their programs, and these attempts 
> at efficiency actually have a strong negative impact when debugging and 
> maintenance are considered."
>
> --- Donald Knuth.

Before you can make your code faster, you first need to figure out what's making it slow. This sounds easy, but it's not. Even experienced programmers have a hard time identifying bottlenecks in their code. Instead of relying on your intuition, you should __profile__ your code: use realistic inputs and measure the run-time of each individual operation. Only once you've identified the most important bottlenecks can you attempt to eliminate them. It's difficult to provide general advice on improving performance, but I try my best with six techniques that can be applied in many situations. I'll also suggest a general strategy for performance optimisation that helps ensure that your faster code will still be correct code.

Once you've identified bottlenecks you'll need to carefully experiment with alternatives to find faster code that is still equivalent. In Chapter  \@ref(perf-improve) you'll learn a bunch of ways to speed up code, but first you need to learn how to __microbenchmark__ so that you can precisely measure the difference in performance.

### Outline {-}

* Section \@ref(profiling) shows you how to use profiling tools to dig into
  exactly what is making a function slow.
  
* Section \@ref(microbenchmarking) shows how to use microbenchmarking to 
  explore alternative implementations and figure out exactly which one is 
  fastest.

### Prerequisites {-}

We'll use [profvis](https://rstudio.github.io/profvis/) for profiling, and [bench](https://bench.r-lib.org/) for microbenchmarking.

```{r setup}
library(profvis)
library(bench)
```

## Profiling {#profiling}
\indexc{RProf()}

Across programming languages, the primary tool used to understand code performance is the profiler. There are a number of different types of profilers ,but R uses a fairly simple type called a sampling or statistical profiler. A sampling profiler stops the execution of code every few milliseconds and records the call stack (i.e. which function is currently executing, and the funcion that function, and so on). For example, consider `f()`, below: 

```{r}
f <- function() {
  pause(0.1)
  g()
  h()
}
g <- function() {
  pause(0.1)
  h()
}
h <- function() {
  pause(0.1)
}
```

(I use `profvis::pause()` instead of `Sys.sleep()` because `Sys.sleep()` does not appear in profiling outputs because as far as R can tell, it doesn't use up any computing time.) \indexc{pause()}

If we profiled the execution of `f()`, stopping the execution of code every 0.1 s, we'd see a profile like below. Each line represents one "tick" of the profiler (0.1 s in this case), and function calls are recorded from right to left: the first line shows `f()` calling `pause()`. It shows that the code spends 0.1 s running `f()`, then 0.2 s running `g()`, then 0.1 s running `h()`.

```
"pause" "f" 
"pause" "f" "g"
"pause" "f" "g" "h"
"pause" "f" "h"
```

If we actually profile `f()`, using `utils::Rprof()` the code below, we're unlikely to get such a clear result.

```{r, eval = FALSE}
tmp <- tempfile()
Rprof(tmp, interval = 0.1)
f()
Rprof(NULL)
writeLines(readLines(tmp))
#> sample.interval=100000
#> "pause" "g" "f" 
#> "pause" "h" "g" "f" 
#> "pause" "h" "f" 
```

That's because profiling has to make a fundamental trade-off between accuracy and performance. The compromise that `RProf()` makes, sampling, only has minimal impact on performance, but is fundamentally stochastic because there's some variability in both the accuracy of the timer and in the time taken by each operation. That means each time that you profile you'll get a slightly different answer. Fortunately, the slowest parts of your code will be least variable, and those are the parts that you're mos interested in.

### Visualising profiles

Typically we'll profile every 0.01 seconds, so we'll generate 100 call stacks per second. That quickly grows beyond our ability to look at directly, so instead of using `uils::Rprof()` we'll use the profvis package to visualise aggregates. It also connects profiling data back to the underlying source code, making it easier to build up a mental model of what you need to change. 

There are other options, like `utils::summaryRprof()`, the proftools package

There are two ways to use profvis:

*   If you're in the RStudio IDE, you can use the "Profile" menu to start 
    profiling, run some code, and then stop it to see the results.
  
*   By calling `profvis::profvis()`. I recommend storing your code in 
    a separate file and source it in; this will ensure you get the best
    connection between profiling data and source code.

    ```{r, eval = FALSE}
    source("profiling-example.R")
    profvis(f())
    ```

After profvis has completed, it will open an interactive HTML document that allows you to explore the results. The top pane shows the source code with a barchart showing the total execution time for each function. This gives you a good overall feel for the bottlenecks but doesn't always help you precisely identify the cause. Here, for example, you can see that `h()` takes 150ms, twice as long as `g()`; that's not because the function itself is slower, but because it's called twice as often.

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("screenshots/performance/flamegraph.png")
```

The bottom pane displays a __flame graph__ which shows the full call stack. This allows you to see the full sequence of calls leading to each function, allowing you to see that `h()` is called from two different places. In this display you can mouse over individual calls to get more information:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("screenshots/performance/info.png")
```

The __data tab__ lets you interactively dive into the tree of performance data. This is basically the same display as the flame graph (rotated 90°), but it's more useful when you have very large or deeply nested call stacks because you can choose to interactively zoom into only selected components.

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("screenshots/performance/tree.png")
```

### Memory profiling
\indexc{read.delim()}

profvis also does memory profiling. We're going to explore a bare-bones implementation of `read.delim()` with only three arguments:

```{r read_delim}
```

We'll also create a sample csv file:

```{r}
write.csv(ggplot2::diamonds, "diamonds.csv", row.names = FALSE)
```

And then profile reading `diamonds.csv()` with `read_delim()`:

```{r, eval = FALSE}
source("memory-read-delim.R")
profvis(read_delim("diamonds.csv"))
```

This yields the same display as before, but now we're going to discuss the memory column:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("screenshots/performance/memory.png")
```

In this example, looking at the allocations tells us most of the story:

* `scan()` allocates about 10 MB of memory, which is very close to the 2.8 MB
  of space that the file occupies on disk. You wouldn't expect the two numbers 
  to be identical because R doesn't need to store the commas and because the 
  global string pool will save some memory.

* Converting the columns allocates another 7 MB of memory. You'd also expect 
  this step to free some memory because we've converted string columns into 
  integer and numeric columns (which occupy less space), but we can't see those 
  releases because GC hasn't been triggered yet.

* Finally, calling `as.data.frame()` on a list allocates about 1.6 megabytes 
  of memory and performs over 600 duplications. This is because 
  `as.data.frame()` isn't terribly efficient and ends up copying the input 
  multiple times. We'll discuss duplication more in the next section.

### Limitations

There are some other limitations to profiling:

* Profiling does not extend to C code. You can see if your R code calls C/C++
  code but not what functions are called inside of your C/C++ code. Unfortunately, 
  tools for profiling compiled code are beyond the scope of this book (i.e., I 
  have no idea how to do it).

* If you're doing a lot of functional programming with anonymous functions,
  it can be hard to figure out exactly which function is being called.
  The easiest way to work around this is to name your functions.

* Lazy evaluation means that arguments are often evaluated inside another 
  function, and this complicates the call stack (Section 
  \@ref(lazy-call-stack)). Unfortunately R's profiler doesn't store enough
  information to disentangle lazy evaluation so that in the following code, 
  profiling would  make it seem like `i()` was called by `j()` because the 
  argument isn'tevaluated until it's needed by `j()`. 

    ```{r, eval = FALSE}
    i <- function() {
      pause(0.1)
      10
    }
    j <- function(x) {
      x + 10
    }
    j(i())
    ```
    
    If this is confusing, you can create temporary variables to force 
    computation to happen earlier.

1.  Since GC is only performed when needed (Section \@ref(gc)), we can never 
    tell exactly when memory is no longer needed. If that is important for 
    your problem, you can force R to GC after every allocation by using 
    `torture = TRUE` (see `gctorture()` for more details). This ensures that
    you see exactly when memory can be freed, but makes R run 10--100x slower.

### Exercises

1.  Profile the following function with `torture = TRUE`. What is 
    surprising? Read the source code of `rm()` to figure out what's going on.

    ```{r}
    f <- function(n = 1e5) {
      x <- rep(1, n)
      rm(x)
    }
    ```

## Microbenchmarking {#microbenchmarking}

A microbenchmark is a measurement of the performance of a very small piece of code, something that might take microseconds (µs) or nanoseconds (ns) to run. I'm going to use microbenchmarks to demonstrate the performance of very low-level pieces of R code, which help develop your intuition for how R works. This intuition, by-and-large, is not useful for increasing the speed of real code. The observed differences in microbenchmarks will typically be dominated by higher-order effects in real code; a deep understanding of subatomic physics is not very helpful when baking. Don't change the way you code because of these microbenchmarks. Instead wait until you've read the practical advice in the following chapters. \index{microbenchmarks}

The best tool for microbenchmarking in R is the bench [@bench] package. It provides very precise timings, making it possible to compare operations that only take a tiny amount of time. For example, the following code compares the speed of two ways of computing a square root.

```{r bench-sqrt}
x <- runif(100)
(lb <- bench::mark(
  sqrt(x),
  x ^ 0.5
))
```

(I save the benchmark to a variable so that I can re-use the numbers in text below. `lb` = last benchmark.)

```{r, dependson = "bench-sqrt", include = FALSE}
sqrt_x <- round(lb$min[[1]], 8)
```

By default, `bench::mark()` runs each expression at least once, and at most enough times to take 0.5s. It returns the results as tibble, with one row for each input expression, and a column for each summary statistic.

* `min`, `mean`, `median`, `max`, and `itr/sec` summarise the time taken by the 
  expression. Focus on the median. In this example, you can see that using the 
  special purpose `sqrt()` function is faster than the general exponentiation 
  operator. 

* `mem_alloc` tells you the amount of memory allocated by the first run,
  and `n_gc()` tells you the total number of garbage collections over all
  runs. These are useful for assessing the memory usage of the expression.
  
* `n_itr` and `total_time` tells you how many times the expression was 
  evaluated and how long that took in total. `n_itr` will always be
  greater than the `min_iteration` parameter, and `total_time` will always
  be greater than the `min_time` parameter.

* `result`, `memory`, `time`, and `gc` are list-columns that store the 
  raw underlying list data.

As with all microbenchmarks, pay careful attention to the units: here, each computation takes about `r ns(sqrt_x)`, `r num(sqrt_x * 1e9)` billionths of a second. To help calibrate the impact of a microbenchmark on run time, it's useful to think about how many times a function needs to run before it takes a second. If a microbenchmark takes:

* 1 ms, then one thousand calls takes a second
* 1 µs, then one million calls takes a second
* 1 ns, then one billion calls takes a second

The `sqrt()` function takes about `r ns(sqrt_x)`, or `r format(sqrt_x * 1e6)` µs, to compute the square root of 100 numbers. That means if you repeated the operation a million times, it would take `r format(sqrt_x * 1e6)` s. So changing the way you compute the square root is unlikely to significantly affect real code.
### Exercises


1. Instead of using `bench::mark()`, you could use the built-in function
   `system.time()`. But `system.time()` is much less precise, so you'll
   need to repeat each operation many times with a loop, and then divide
   to find the average time of each operation, as in the code below.

    ```{r, eval = FALSE}
    n <- 1e6
    system.time(for (i in 1:n) sqrt(x)) / n
    system.time(for (i in 1:n) x ^ 0.5) / n
    ```
    
    How do the estimates from `system.time()` compare to those from
    `bench::mark()`? Why are they different?

1.  Here are two other ways to compute the square root of a vector. Which
    do you think will be fastest? Which will be slowest? Use microbenchmarking
    to test your answers.

    ```{r, eval = FALSE}
    x ^ (1 / 2)
    exp(log(x) / 2)
    ```

1.  Use microbenchmarking to rank the basic arithmetic operators (`+`, `-`,
    `*`, `/`, and `^`) in terms of their speed. Visualise the results. Compare
    the speed of arithmetic on integers vs. doubles.
