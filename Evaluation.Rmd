# Evaluation

```{r, include = FALSE}
source("common.R")
```

## Introduction

The user-facing opposite of quotation is unquotation: it gives the _user_ the ability to selectively evaluate parts of an otherwise quoted argument. The developer-facing complement of quotation is evaluation: this gives the _developer_ of the function the ability to evaluate quoted expressions in special ways to create domain specific languages for data analysis like ggplot2 and dplyr.

Mention tidy evaluation because it's one of the principles underlying the tidyverse. Why do I teach here when it's not base R? Because it gives us the following good things.

Tidy evaluation is the combination of four big ideas:

* Quasiquotation to give the user control of quoting.

* Quosures to capture arguments expressions and their evaluation environment.

* Data masks to enable expressions to mingle variables from an environment
  and from a data source.

* Pronouns to eliminate ambiguity between environment and data source when
  needed.

```{r setup}
library(rlang)
```

##### Outline

##### Prerequisites

Environments play a big role in evaluation, so make sure you're familiar with [Environments] before continuing.

## Evaluation basics

In the previous chapter, we briefly mentioned `eval()`. Here, rather than starting with `eval()`, we're going to start with `rlang::eval_bare()` which is the purest evocation of the idea of evaluation. The first argument, `expr` is an expression to evaluate. This will usually be either a symbol or expression:

```{r}
x <- 10
eval_bare(expr(x))

y <- 2
eval_bare(expr(x + y))
```

The second argument, `env`, gives the environment in which the expression should be evaluated, i.e. where should the values of `x`, `y`, and `+` be looked for? By default, this is the current environment, i.e. the calling environment of `eval_bare()`, but you can override it if you want:

```{r}
eval_bare(expr(x + y), env(x = 1000))
```

Because R looks up functions in the same way as variables, we can also override the meaning of functions. This is a key technique for generating DSLs, as discussed in the next chapter.

```{r}
eval_bare(expr(x + y), env(`+` = function(x, y) paste0(x, " + ", y)))
```

If passed an object other than a symbol or expression, the evaluation functions will simply return the input as is (because it's already evaluated). This can lead to confusing results if you forget to `quote()` the input: `eval_bare()` doesn't quote `expr` so it is passed by value.

```{r}
eval_bare(x + y)
eval_bare(x + y, env = env)
```

Now that you've seen the basics, let's explore some applications. We'll focus primarily on base R functions that you might have used before; now you can learn how they work. To focus on the underlying principles, we'll extracting their essence and rewrite to use functions from rlang. We'll then circle back and talk about the base R functions most important for evaluation.

### Application: `local()`

Sometimes you want to perform a chunk of calculation that creates a bunch of intermediate variables. The intermediate variables have no long term use and could be quite large, so you'd rather not keep them around. One approach is to clean up after yourself using `rm()`. Another approach is to wrap the code in a function, and just call it once.

A more elegant approach is to use `local()`:

```{r, error = TRUE}
# Clean up variables created earlier
rm(x, y)

foo <- local({
  x <- 10
  y <- 200
  x + y
})

foo
x
y
```

The essence of `local()` is quite simple. We capture the expression, and create an new environment in which to evaluate it. This environment inherits from the caller environment so it can access the current lexical scope.

```{r, error = TRUE}
local2 <- function(expr, env = child_env(caller_env())) {
  eval_bare(enexpr(expr), env)
}

foo <- local2({
  x <- 10
  y <- 200
  x + y
})

env_has(nms = c("x", "y"))
```

It's a bit harder to understand how `base::local()` works, as it takes uses `eval()` and `substitute()` together in rather complicated ways. Figuring out exactly what's going on is good practice if you really want to understand the subtleties of `substitute()` and the base `eval()` funtions.

### Application: `source()`

We can create a simple version of `source()` by combining `expr_text()` and `eval_tidy()`. We read in the file from disk, use `parse_expr()` to parse the string into an list of expressions, and then use `eval_bare()` to evaluate each component. This version evaluates the code in the caller environment, and invisibly returns the result of the last expression in the file (like `source()`). \index{source()}

```{r}
source2 <- function(file, env = caller_env()) {
  lines <- readLines(file, warn = FALSE)
  code <- paste(lines, collapse = "\n")
  exprs <- parse_exprs(code)

  res <- NULL
  for (i in seq_along(exprs)) {
    res <- eval_bare(exprs[[i]], env)
  }
  
  invisible(res)
}
```

The real `source()` is considerably more complicated because it can `echo` input and output, and also has many additional settings to control behaviour. 

### Base R

The base function equivalent to `eval_bare()` is the two-argument form of `eval()`: `eval(expr, envir)`: 

```{r}
eval(expr(x + y), env(x = 1000, y = 1))
```

The final argument, `enclos` provides support for data masks, which you'll learn about in [tidy evaluation]. 

`eval()` is paired with two helper functions: 

* `evalq(x, env)` quotes its first argument, and is hence a shortcut for 
  `eval(quote(x), env)`.

* `eval.parent(expr, n)` is shortcut for `eval(x, env = parent.frame(n))`.

`base::eval()` has special behaviour for expression __objects__, evaluating each component in turn. This makes for a very compact implementation of `source2()` because `base::parse()` also returns an expression object:

```{r}
source3 <- function(file, env = parent.frame()) {
  lines <- parse(file)
  res <- eval(lines, envir = env)
  invisible(res)
}
```

While `source3()` is considerably more concise than `source2()`, this one use case is the strongest argument for expression objects, and overall we don't believe this one benefit outweighs the cost of introducing a new data structure.

### Exercises

1.  Carefully read the documentation for `source()`. What environment does it
    use by default? What if you supply `local = TRUE`? How do you provide 
    a custom argument?

1.  Predict the results of the following lines of code:

    ```{r, eval = FALSE}
    eval(quote(eval(quote(eval(quote(2 + 2))))))
    eval(eval(quote(eval(quote(eval(quote(2 + 2)))))))
    quote(eval(quote(eval(quote(eval(quote(2 + 2)))))))
    ```

1.  Write an equivalent to `get()` using `sym()` and `eval_bare()`. Write an
    equivalent to `assign()` using `sym()`, `expr()`, and `eval_bare()`.
    (Don't worry about the multiple ways of choosing an environment that
    `get()` and `assign()` support; assume that the user supplies it 
    explicitly.)
    
    ```{r}
    # name is a string
    get2 <- function(name, env) {}
    assign2 <- function(name, value, env) {}
    ```

1.  Modify `source2()` so it returns the result of _every_ expression,
    not just the last one. Can you eliminate the for loop?

1.  The code generated by `source2()` lacks source references. Read
    the source code for `sys.source()` and the help for `srcfilecopy()`,
    then modify `source2()` to preserve source references. You can
    test your code by sourcing a function that contains a comment. If
    successful, when you look at the function, you'll see the comment and
    not just the source code.

1.  The third argument in `subset()` allows you to select variables. It
    treats variable names as if they were positions. This allows you to do 
    things like `subset(mtcars, , -cyl)` to drop the cylinder variable, or
    `subset(mtcars, , disp:drat)` to select all the variables between `disp`
    and `drat`. How does this work? I've made this easier to understand by
    extracting it out into its own function that uses tidy evaluation.

    ```{r, eval = FALSE}
    select <- function(df, vars) {
      vars <- enexpr(vars)
      var_pos <- set_names(as.list(seq_along(df)), names(df))
      
      cols <- eval_tidy(vars, var_pos)
      df[, cols, drop = FALSE]
    }
    select(mtcars, -cyl)
    ```

1.   We can make `base::local()` slightly easier to understand by spreading
    out over multiple lines:

    ```{r}
    local3 <- function(expr, envir = new.env()) {
      call <- substitute(eval(quote(expr), envir))
      eval(call, envir = parent.frame())
    }
    ```
    
    Explain how `local()` works in words. (Hint: you might want to `print(call)`
    to help understand what `substitute()` is doing, and read the documentation
    to remind yourself what environment `new.env()` will inherit from.)
    
## Quosures

The simplest form of evaluation combines an expression and an environment. This coupling is sufficiently important that we need a data structure that captures both pieces. We call this data structure a __quosure__, a portmanteau of quoting and closure.

### Motivation

Quosures are needed when expressions to be evaluate mix variables from a data frame and variables in the environment. For example, the following `mutate()` call creates a new variable called `log` with a calculation that involves a varible in the dataset `x`, and a variable in the environment, `base`:

```{r}
df <- data.frame(z = runif(5))
x <- 10
dplyr::mutate(df, log = log(z, base = x))
```

Also remember that `log()` itself is found in the global environment, but there's no confusion about where functions come from because (without gymnastics) you can't put a function in a data frame.

Worrying about the execution environment of an argument is important when you write quoting functions. Take this simple example:

```{r}
compute_mean <- function(df, x) {
  x <- enexpr(x)
  dplyr::summarise(df, mean = mean(!!x))
}
```

It works correctly for simple inputs:

```{r}
compute_mean(df, z)
```

It contains a subtle bug, which we can illustrate with this slightly forced example:

```{r, error = TRUE}
x <- 10
compute_mean(df, log(z, base = x))
```

We get this error because we have lost the evaluation environment associated with `log(z, base = x)` so it is evaluated inside `compute_mean()` where is `x` an AST. This type of bug is pernicious because it will happen rarely and the error message will be inscrutable.

We can avoid the bug by the expression along with its evaluation evniroment. That's the job of `enquo()`, which otherwise works identically to `enexpr()`:

```{r}
compute_mean <- function(df, x) {
  x <- enquo(x)
  dplyr::summarise(df, mean = mean(!!x))
}

compute_mean(mtcars, log(mpg, base = x))
```

### Creating and manipulating

To create a quosure you will typically use one of the equivalents of the `expr()` functions that you learned about in the previous chapter:

* Use `quo()` and `quos()` for experimenting interactively and for 
  unquoting with fixed expressions inside a function.
  
* Use `enquo()` and `enquos()` to capture user-supplied arguments to a function.

Alternatively, you can use `new_quosure()` to create a quosure from its components: an expression and an environment.

```{r}
x <- new_quosure(expr(x + y), env(x = 1, y = 10))
x
```

Note how quosures are printed. If you unquote a quosure inside another quosure, each quosusre starts with `^` and if you're in console that supports it, each quosure gets a differnt colour to help remind you that it has a different environment attached to it.

```{r}
q2 <- quo(x + !!x)
q2
```

You can evaluated a quosure with `eval_tidy()`, which we'll study in depth in the next chapter:

```{r}
eval_tidy(x)
```

You can the extract components with `quo_` helpers:

```{r}
quo_get_env(x)
quo_get_expr(x)
```

And if you need to turn a quosure into text for output to the console you can use `quo_name()`, `quo_label()`, or `quo_text()`. `quo_name()` and `quo_label()` are garanteed to be short; `quo_expr()` may span multiple lines.

```{r}
# https://github.com/tidyverse/rlang/issues/367
y <- quo(long_function_name(
  argument_1 = long_argument_value,
  argument_2 = long_argument_value,
  argument_3 = long_argument_value,
  argument_4 = long_argument_value
))
quo_name(y)   # e.g. for data frames
quo_label(y)  # e.g. for error messages
quo_text(y)   # for longer messages
```

### Implementation

Quosures are possible because internally R represents function arguments with a special type of object called a __promise__. A promise captures the expression needed to compute the value and the environment in which to compute it. You're not normally aware of promises because the first time you access a promise its code is evaluated in its environment, yielding a value. This is what powers lazy evaluation. \index{promises}

However, you cannot manipulate promises with R code: they're sort of quantum; if you attempt to manipulate with R code, they are immediately evaluated, and the promise nature goes away. To work around this, rlang manipulates the promise in C, reifying it into an R object that you can work with.

There is one big difference between promises and quosures. An argument is evaluated implicitly when you access it for the first time. Every time you access it subsequently it will return the same value. A quosure must be evaluated explicitly, and each evaluation is independent of the previous evaluations.

```{r}
# The argument x is evaluated once, then reuses
foo <- function(x_arg) {
  list(x1 = x_arg, x2 = x_arg)
}
foo(runif(3))

# The quosure x is evaluated afresh each time
x_quo <- quo(runif(3))
eval_tidy(x_quo)
eval_tidy(x_quo)
```

Quosures are inspired by the the formula operator, `~`, which also captures both the expression and its environment, and is used extremely heavily in R's modelling functions:

```{r}
f <- ~runif(3)
f

str(f)
```

Initial versions of rlang used formulas as quosures: an attractive feature of `~` is that is provides quoting with just a single keystroke. Unfortunately, however, there is no way to add quasiquotation to `~`, so we decided to use a new function, `quo()`, instead.

### Exercises

1.  Predict what evaluating each of the following quosures will return.

    ```{r}
    q1 <- new_quosure(expr(x), env(x = 1))
    q2 <- new_quosure(expr(x + !!q1), env(x = 10))
    q3 <- new_quosure(expr(x + !!q2), env(x = 100))
    ```

1.  Run this code in your head and predict what it will print. Confirm or 
    refute your prediction by running the code in R.

    ```{r, results = FALSE}
    f <- function(...) {
      x <- "f"
      g(f = x, ...)
    }
    g <- function(...) {
      x <- "g"
      h(g = x, ...)
    }
    h <- function(...) {
      enquos(...)
    }
    x <- "top"
    
    out <- f(top = x)
    out
    purrr::map_chr(out, eval_tidy)
    ```

## Tidy evaluation

In the previous section, you saw why quosures are important when calling other functions that use tidy evaluation. In this section, 

Learned about quasiquotation to give the user control of quoting, and quosures to capture arguments expressions and their evaluation environment. Next we'll learn about data masks and pronouns. These make it easy to evaluate an expression in the context of a data frame.

You've learned about quaisquotation and quosures, now time to learn about the data mask, pronouns and why they're important. Along the way, you'll also learn how to create your own functions that use tidy evaluation.

You almost always want to capture a quosure rather than an expression because it gives you uniformly more information. Once we've discussed the primary use case of tidy evaluation, we'll come back to the few cases where you should prefer expressions.

### `eval_tidy()`

Once you have a quosure, you will need to use `eval_tidy()` instead of `eval_bare()`:

```{r}
x <- 10

# These two calls are equivalent
eval_bare(expr(x), globalenv())
eval_tidy(quo(x))
```

Like `eval_bare()`, `eval_tidy()` has a `env` argument, but generally you will not use it, because the environment is captured by the quosure. Instead, you will typically use the second argument, `data`. This lets you set up a __data mask__, where variables in the environment are potentially masked by variables in data frame. This allows you to mingle variables from the environment and variables from a data frame: 

```{r}
eval_tidy(quo(cyl + x), mtcars)
```

This is the key idea that powers base R functions like `with()`, `subset()` and `transform()`, and that is used through tidyverse packages like dplyr.

### Data masks

Unlike environments, list and data frames don't have parent-child relationships. When you use the `data` argument (of the `enclos` argument in `base::eval()`) you're effectively create a new environment that contains the values of `data` and has a parent of `env`.

INSERT DIAGRAM HERE

```{r}
q1 <- quo(cyl + x)
# eval_tidy(q1, mtcars) is equivalent to:

q2 <- quo_set_env(q1, as_env(mtcars, quo_get_env(q1)))
eval_tidy(q2)
```

### Application: `subset()`

To see why the data mask is so important, lets implement our own version of `subset()`. Later we'll come back to the base `subset()` function, and discuss how it works, and why it's documentation contains such a strong caution not to use it.

If you haven't used it before `subset()` (like `dplyr::filter()`), provides a convenient way of selecting rows of a data frame based using an expression that is evaluated in the context of the data frame. It allows you to subset a data frame without repeatedly doing `df$`.

```{r}
sample_df <- data.frame(a = 1:5, b = 5:1, c = c(5, 3, 1, 4, 1))

# sample_df[sample_df$a >= 4, ]
subset(sample_df, a >= 4)

# sample_df[sample_df$b == sample_df$c, ]
subset(sample_df, b == c)
```

The core of is quite simple: we capture the `subset` expression, evaluate it in with a data mask, and use the results to subset the data frame. I've included a very simple check to ensure the result is a logical vector; real code should do more work to deliver an informative error to the user.

```{r}
subset2 <- function(data, subset) {
  subset <- enquo(subset)
  rows <- eval_tidy(subset, data)
  stopifnot(is.logical(rows))
  
  data[rows, , drop = FALSE]
}

subset(sample_df, b == c)
```

Because we've use `enquo()`, this function will support quasiquotation:

```{r}
var <- expr(b)
val <- 5

subset2(sample_df, !!var == val)
```

### Lexical scoping, ambiguity, and pronouns

One of the downsides of the data mask is that it introduces ambiguity in the expression: when you say `x` do you mean the variable or the column? This ambiguity is usually fine when doing interactive data analysis because you are familiar with the variables, and because you are looking at the data frequently you will tend to spot problems quickly. The downside of ambiguity is worth it for terser code.

However, ambiguity can start to be a problem if you wrap tidy evaluation functions inside another function. For example, take this simple wrapper:

```{r}
threshold_x <- function(df, val) {
  subset2(df, x >= val)
}
```

How can this function fail? There are two main ways:

*   `df` might not contain a variable called `x`. Depending on what variables 
    exist in the global environment this might either return the incorrect
    results:

    ```{r}
    no_x <- data.frame(y = 1:3)
    threshold_x(no_x, 2)
    ```
   
    Or throw an error:
  
    ```{r, error = TRUE}
    rm(x)
    threshold_x(no_x, 2)
    ```

*   `df` might contain a variable called `val`, in which case the function will
    silently return an incorrect value:
   
    ```{r}
    has_val <- data.frame(x = 1:3, val = 9:11)
    threshold_x(has_val, 2)
    ```

These failure modes arise because tidy evaluation is ambiguous: for each variable look up, it looks first in the data and then in the environment. But in this case, we always want to look up `x` in the data and `val` in the environment. To avoid this problem we can use the `.data` and `.env` pronouns:

```{r}
threshold_x <- function(df, val) {
  subset2(df, .data$x >= .env$val)
}
```

(Note that unlike indexing an ordinary list or environment with `$`, if the variable is not found then these pronouns will throw an error)

```{r, error = TRUE}
x <- 10
threshold_x(no_x, 2)
threshold_x(has_val, 2)
```

Generally, whenever you use the `.env` pronoun, you can use unquoting instead:

```{r}
threshold_x <- function(df, val) {
  subset2(df, .data$x >= !!val)
}
```

There are subtle differences in when `val` is evaluated. If you unquote, it is evaluated by the quoting function (e.g `enquo()`); if you use a pronoun, it is evaluated by `eval_tidy()`. These differences are usually unimportant, so pick the form that looks most natural.

What if we generalise `threshold_x()` slightly so that the user can pick the variable used for thresholding. There are two basic approaches:

```{r}
threshold <- function(df, var, val) {
  var <- ensym(var)
  subset2(df, `$`(data, !!var) >= !!val)
}

threshold <- function(df, var, val) {
  var <- as.character(ensym(var))
  subset2(df, data[[!!var]] >= !!val)
}
```

* Both now involve capturing a symbol. Things fundamentally change if we capture
  an expression and we'll see next.

* `df$!!var` is not valid R syntax; we have to use prefix form.
  Alternatively we can use `[[` and supply a string instead.


Note that it is not always the responsibility of the function author to avoid ambiguity. Imagine we generalise further to allow thresholding based on any expression. You could write:

```{r}
threshold <- function(df, expr, val) {
  expr <- enquo(expr)
  subset2(df, !!expr >= !!val)
}
```

There's no way to ensure that `expr` is only evaluated in the `data`, and indeed that might not even be desirable because the user may use an expression that includes variables from the data and from the local environments. In this case, it is now the users responsibility to avoid ambiguity. 

(But this specific function is now not very useful because it's so general - you might as well just use `subset2()` directly.)

### Base R

`enclos` argument.

The documentation of `subset()` includes the following warning:

> This is a convenience function intended for use interactively. For 
> programming it is better to use the standard subsetting functions like `[`, 
> and in particular the non-standard evaluation of argument `subset` can have 
> unanticipated consequences.

Why?

No unquoting so wrapping hard

```{r, error = TRUE}
f1 <- function(df, expr) {
  subset(df, expr)
}
df <- data.frame(x = c(1, 10))
f1(df, x == 1)

f2 <- function(df, expr) {
  eval(substitute(subset(df, expr)), parent.frame())
}
f2(df, x == 1)
```

Always evaluates arguments in parent frame:

```{r}
f <- function(df, ...) {
  xval <- 10
  subset(df, ...)
}

xval <- 1
f(df, x == xval)
```

No pronouns, so no way to safely write the equivalent of `threshold_x()`.

Why is using `[` not adequate?  Because although it is simple, `subset()` provides two useful behaviours:

* It sets `drop = FALSE` by default, so it's garuanteed to return a data frame
* It drops rows where the conditional evaluates to `NA`.

That means `subset(df, x == y)` is not equivalent to `df[x == y,]` but is equivalent to `df[x == y & !is.na(x == y), , drop = FALSE]`.

### Application: `arrange()`

* Capture dots
* Evaluate
* Combine
* Subset

```{r}
invoke <- function(fun, ...) do.call(fun, dots_list(...))

arrange <- function(.data, ..., .na.last = TRUE) {
  args <- quos(...)
  
  ords <- purrr::map(args, eval_tidy, data = .data)
  ord <- invoke(order, !!!ords, na.last = .na.last)
  
  .data[ord, , drop = FALSE]
}

arrange(mtcars, cyl)
arrange(mtcars, vs, -am)
```

Missing: any error checking. Should at least check that each input yields a vector the same length as `.data`.

### Multiple environments

Note that when using `...` each component can have a different environment associated with it:

```{r}
f <- function(...) {
  x <- 1
  g(..., x1 = x)
}
g <- function(...) {
  x <- 2
  h(..., x2 = x)
}
h <- function(...) {
  enquos(...)
}

x <- 0
qs <- f(x0 = x)
qs
```

```{r}
purrr::map(qs, quo_get_expr)
purrr::map(qs, quo_get_env)
purrr::map_dbl(qs, eval_tidy)
```

### Embedded quosures

```{r}
make_x <- function(x) quo(x)
thirty <- quo(!!make_x(0) + !!make_x(10) + !!make_x(20))
thirty
```

(Note that because quosures capture the complete environment you need to be a little careful if your function returns quosures. If you have large temporary objects they will not get gc'd until the quosure has been gc'd. See XXXXXXX for more details.)

If you're viewing from the console, you'll see that each quosure is coloured - the point of the  colours is to emphasise that the quosures have different environments associated with them even though the expressions are the same.

```{r}
eval_tidy(thirty)
```

This was a lot of work to get right. But means that quosures just work, even when embedded inside other quosures.

Note that this code doesn't make any sense at all if we use expressions instead of quosures equivalents, the environment is never captured so all we have

```{r}
make_x <- function(x) expr(x)
thirty <- expr(!!make_x(0) + !!make_x(10) + !!make_x(20))

thirty
eval_tidy(thirty)
```


### Performance

Note that there is a performance overhead to `eval_tidy()

```{r}
n <- 1000
x1 <- expr(runif(n))
e1 <- globalenv()
q1 <- quo(runif(n))

microbenchmark::microbenchmark(
  runif(n),
  eval_bare(x1, e1),
  eval_tidy(q1),
  eval_tidy(q1, mtcars)
)
```

However, most of that extra overhead is due to setting up the data mask so if you need to evaluate code repeatedly, it's worthwhile to do the coercion to data mask once, and then reuse. This substantially reduces the overhead.

```{r}
d_mtcars <- as_data_mask(mtcars)

microbenchmark::microbenchmark(
  as_data_mask(mtcars), 
  eval_bare(x1, e1),
  eval_tidy(q1, d_mtcars)
)
```

(The amount of savings is surprising: https://github.com/tidyverse/rlang/issues/372)

### When not to use quosures

* In code generation.

* When expression will be evaluated completely in data context

* To call functions that don't use tidy eval; fuller example next.

Sometimes you can avoid using a quosure by inlining/unquoting values.

```{r}
base <- 2
quo(log(x, base = base))
expr(log(x, base = !!base))
```

### Exercises

1.  Improve `subset2()` to make it more like real subset function 
    (`subset.data.frame()`):
    
    * All drop rows where `subset` evaluates to `NA`
    * Give a clear error message if `subset` doesn't evalute to a logical vector
    * What happens if `subset` doesn't yield a logical vector with length
      equal to the number of rows in `data`? What do you think should happen?

1.  What happens if you use `expr()` instead of `enexpr()` inside of
    `subset2()`?

1.  Implement a form of `arrange()` where you can request a variable to 
    sorted in descending order using named arguments:
    
    ```{r, eval = FALSE}
    arrange(mtcars, cyl, desc = mpg, vs)
    ```
    
    (Hint:  The `descreasing` argument to `order()` will not help you. Instead,
    look at the definition of `dplyr::desc()`, and read the help for `xtfrm()`.)

1.  What does `transform()` do? Read the documentation. How does it work?
    Read the source code for `transform.data.frame()`. What does
    `substitute(list(...))` do?

1.  What does `with()` do? How does it work? Read the source code for
    `with.default()`. What does `within()` do? How does it work? Read the
    source code for `within.data.frame()`. Why is the code so much more
    complex than `with()`?

1.  Implement `with()` (code in `with.default()`).

1.  Implement a version of `within.data.frame()` that uses tidy evaluation.
    Read the documentation and make sure that you understand what `within()`
    does, then read the source code.

1.  Implement `transform()` (code in `transform.data.frame()`).  Extend it so that a
    variable can refer to the variables just defined.

## Case study: calling base NSE functions

We can combine `expr()` with `eval_bare()` to create wrappers around base NSE functions that don't provide an escape hatch for quoting. Here we'll focus on models, since since standard NSE doesn't provide unquoting tool. But can use the same ideas with base graphics and any other function.

### Basics

`lm()` is particularly challenging because it captures and prints the actual call.  Ideally we want this to be useful.

```{r}
lm2 <- function(data, formula, subset = NULL) {
  data <- enexpr(data)
  subset <- enexpr(subset)
  
  lm_call <- expr(lm(!!formula, data = !!data, subset = !!subset))
  eval_bare(lm_call, caller_env())
}
coef(lm2(mtcars, mpg ~ disp))
coef(lm2(mtcars, mpg ~ disp, subset = cyl == 4))
```

### What environment to use

### Missing vs NULL

I think it's good practice to only leave missing the arguments that the user must supply. Instead, use `NULL` - has nice property that `expr(NULL)` is `NULL`. Then can use `%||%` and `missing_arg()` to replace, if needed. One final wrinkle is that unquoting a missing argument will yield an error about the missing argument; wrap in `maybe_missing()` to suppress

```{r}
lm3 <- function(data, formula, subset = NULL) {
  data <- enexpr(data)
  subset <- enexpr(subset) %||% missing_arg()
  
  lm_call <- expr(lm(!!formula, data = !!data, subset = !!maybe_missing(subset)))
  eval_bare(lm_call, caller_env())
}
lm2(mtcars, mpg ~ disp)$call
lm3(mtcars, mpg ~ disp)$call
```

### Making formulas

First let's show how you could generate a formula. Tricky thing about formulas is that the look same evaluated or not

```{r}
y ~ x
expr(y ~ x)
```

But they're not - you need to evaluate the call to get an actual formula:

```{r}
class(y ~ x)
class(expr(y ~ x))
```

Here's a simple example of generating a formula in a different way:

```{r}
build_formula <- function(resp, ..., env = caller_env()) {
  resp <- enexpr(resp)
  preds <- enexprs(...)
  
  pred_sum <- purrr::reduce(preds, ~ expr(!!.x + !!.y))
  eval_bare(expr(!!resp ~ !!pred_sum), env = env)
}
build_formula(y, a, b, c)
```

Can use the techniques described in the previous chapter to allow you to choose the interface to this function.

### Exercises


## Capturing the current call {#capturing-call}

(Where should this go???)

```{r, eval = FALSE, echo = FALSE}
std <- c("package:base", "package:utils", "package:stats")
names(find_uses(std, "sys.call"))
names(find_uses(std, "match.call"))
```

Many base R functions use the current call: the expression that caused the current function to be run. There are two ways to capture a current call: \indexc{calls|capturing current}

* `sys.call()` captures exactly what the user typed. \indexc{sys.call()}

* `match.call()` makes a call that only uses named arguments. It's like
  automatically calling `pryr::standardise_call()` on the result of
  `sys.call()` \indexc{match.call()}

The following example illustrates the difference between the two:

```{r}
f <- function(abc = 1, def = 2, ghi = 3) {
  list(sys = sys.call(), match = match.call())
}
f(d = 2, 2)
```

Modelling functions often use `match.call()` to capture the call used to create the model. This makes it possible to `update()` a model, re-fitting the model after modifying some of original arguments. Here's an example of `update()` in action: \indexc{update()}

```{r}
mod <- lm(mpg ~ wt, data = mtcars)
update(mod, formula = . ~ . + cyl)
```

How does `update()` work? We can rewrite it using some tools from pryr to focus on the essence of the algorithm.

```{r, eval = FALSE}
update_call <- function (object, formula., ...) {
  call <- object$call

  # Use update.formula to deal with formulas like . ~ .
  if (!missing(formula.)) {
    call$formula <- update.formula(formula(object), formula.)
  }

  modify_call(call, dots(...))
}
update_model <- function(object, formula., ...) {
  call <- update_call(object, formula., ...)
  eval(call, parent.frame())
}
update_model(mod, formula = . ~ . + cyl)
```

The original `update()` has an `evaluate` argument that controls whether the function returns the call or the result. But I think it's better, on principle, that a function returns only one type of object, rather than different types depending on the function's arguments.

This rewrite also allows us to fix a small bug in `update()`: it re-evaluates the call in the global environment, when what we really want is to re-evaluate it in the environment where the model was originally fit --- in the formula.

```{r, error = TRUE}
f <- function() {
  n <- 3
  lm(mpg ~ poly(wt, n), data = mtcars)
}
mod <- f()
update(mod, data = mtcars)

update_model <- function(object, formula., ...) {
  call <- update_call(object, formula., ...)
  eval(call, environment(formula(object)))
}
update_model(mod, data = mtcars)
```

This is an important principle to remember: if you want to re-run code captured with `match.call()`, you also need to capture the environment in which it was evaluated, usually the `parent.frame()`. The downside to this is that capturing the environment also means capturing any large objects which happen to be in that environment, which prevents their memory from being released. This topic is explored in more detail in [garbage collection](#gc). \index{environments|capturing}

Some base R functions use `match.call()` where it's not necessary. For example,  `write.csv()` captures the call to `write.csv()` and mangles it to call `write.table()` instead:

```{r}
write.csv <- function(...) {
  Call <- match.call(expand.dots = TRUE)
  for (arg in c("append", "col.names", "sep", "dec", "qmethod")) {
    if (!is.null(Call[[arg]])) {
      warning(gettextf("attempt to set '%s' ignored", arg))
    }
  }
  rn <- eval.parent(Call$row.names)
  Call$append <- NULL
  Call$col.names <- if (is.logical(rn) && !rn) TRUE else NA
  Call$sep <- ","
  Call$dec <- "."
  Call$qmethod <- "double"
  Call[[1L]] <- as.name("write.table")
  eval.parent(Call)
}
```

To fix this, we could implement `write.csv()` using regular function call semantics:

```{r}
write.csv <- function(x, file = "", sep = ",", qmethod = "double", 
                      ...) {
  write.table(x = x, file = file, sep = sep, qmethod = qmethod, 
    ...)
}
```

This is much easier to understand: it's just calling `write.table()` with different defaults. This also fixes a subtle bug in the original `write.csv()`: `write.csv(mtcars, row = FALSE)` raises an error, but `write.csv(mtcars, row.names = FALSE)` does not. The lesson here is that it's always better to solve a problem with the simplest tool possible.

### Exercises

1.  Compare and contrast `update_model()` with `update.default()`.

1.  Why doesn't `write.csv(mtcars, "mtcars.csv", row = FALSE)` work?
    What property of argument matching has the original author forgotten?

1.  Rewrite `update.formula()` to use R code instead of C code.

1.  Sometimes it's necessary to uncover the function that called the
    function that called the current function (i.e., the grandparent, not
    the parent). How can you use `sys.call()` or `match.call()` to find
    this function?
